Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('6','2016','long','Strategies for Training Large Vocabulary Neural Language Models','Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('7','2016','long','Multimodal Pivots for Image Caption Translation','We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolingually captioned images, and on state-of-the-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('8','2016','short','Science Question Answering using Instructional Materials','We provide a solution for elementary science test using instructional materials. We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs and instructional materials), and uses what it learns to answer novel elementary science questions. Our evaluation shows that our framework outperforms several strong baselines.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('9','2016','long','Text Understanding with the Attention Sum Reader Network','Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children''s Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('10','2016','long','A Fast Unified Model for Parsing and Sentence Understanding','Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they usually operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25 times over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('11','2016','long','Globally Normalized Transition-Based Neural Networks','We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('12','2016','long','Harnessing Deep Neural Networks with Logic Rules','Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('13','2016','long','Incorporating Copying Mechanism in Sequence-to-Sequence Learning','We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('14','2016','long','Latent Predictor Networks for Code Generation','Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('15','2016','long','Neural Summarization by Extracting Sentences and Words','Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('16','2016','short','Nonparametric Spherical Topic Modeling with Word Embeddings','Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('17','2016','long','Using Sentence-Level LSTM Language Models for Script Inference','There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('18','2016','long','Universal Dependencies for Learner English','We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language. The treebank is available at universaldependencies.org. The annotation manual used in this project and a graphical query engine are available at esltreebank.org.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('19','2016','short','Syntactically Guided Neural Machine Translation','We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('20','2016','long','Neural Semantic Role Labeling with Dependency Path Embeddings','This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as sub-sequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('21','2017','long','Gated-Attention Readers for Text Comprehension','In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN and Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at https://github.com/bdhingra/ga-reader.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('22','2016','long','Learning Language Games through Interaction','We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein''s language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer''s capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans'' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('23','2016','long','Coordination Annotation Extension in the Penn Tree Bank','Coordination is an important and common syntactic construction which is not handled well by state of the art parsers. Coordinations in the Penn Treebank are missing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsistencies. In this work, we initiated manual annotation process for solving these issues. We identify the different elements in a coordination phrase and label each element with its function. We add phrase boundaries when these are missing, unify inconsistencies, and fix errors. The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations. We make the coordination annotation publicly available, in hope that they will facilitate further research into coordination disambiguation.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('38','2017','long','Morphological Inflection Generation with Hard Monotonic Attention','We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft attention \cite{bahdanauCB14} models for the task, shedding some light on the features such models extract.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('39','2016','long','Harnessing Cognitive Features for Sarcasm Detection','In this paper, we propose a novel mechanism for enriching the feature vector, for the task of sarcasm detection, with cognitive features extracted from eye-movement patterns of human readers. Sarcasm detection has been a challenging research problem, and its importance for NLP applications such as review summarization, dialog systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the behaviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by his observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of F-score), over the performance of the best reported system.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('40','2017','short','AMR-to-text Generation with Synchronous Node Replacement Grammar','This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('41','2017','long','Neural Discourse Structure for Text Categorization','We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('42','2017','long','MORSE: Semantic-ally Drive-n MORpheme SEgment-er','We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('43','2017','short','Learning to Parse and Translate Improves Neural Machine Translation','There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('44','2017','long','Reading Wikipedia to Answer Open-Domain Questions','This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('45','2017','long','One-Shot Neural Cross-Lingual Transfer for Paradigm Completion','We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('46','2017','short','Multi-Task Learning of Keyphrase Boundary Classification','Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('47','2017','long','A Transition-Based Directed Acyclic Graph Parser for UCCA','We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('48','2017','long','A Syntactic Neural Model for General-Purpose Code Generation','We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('49','2017','long','What do Neural Machine Translation Models Learn about Morphology?','Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('50','2017','long','Learning Character-level Compositionality with Visual Features','Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry semantic content, resulting in embeddings that are coherent in visual space.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('51','2017','long','An Interpretable Knowledge Transfer Model for Knowledge Base Completion','Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, \emph{ITransF}, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets---WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('52','2017','long','Neural End-to-End Learning for Computational Argumentation Mining','We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning ''natural'' subtasks, in a multi-task learning setup, improves performance.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('53','2017','long','Improved Neural Relation Detection for Knowledge Base Question Answering','Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('54','2017','long','Bandit Structured Prediction for Neural Sequence-to-Sequence Learning','Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('55','2017','short','Improving Semantic Composition with Offset Inference','Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers missing data by the same mechanism that is used for semantic composition.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('56','2017','short','Lexical Features in Coreference Resolution: To be Used With Caution','Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('57','2017','long','Argument Mining with Structured SVMs and RNNs','We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('58','2017','long','Learning to Skim Text','Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q and A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('59','2017','long','Deep Keyphrase Generation','Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('60','2017','long','Neural Machine Translation via Binary Code Prediction','In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('61','2017','long','Naturalizing a Programming Language via Interactive Learning','Our goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to "naturalize" the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9% of the last 10K utterances.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('62','2017','long','Translating Neuralese','Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents'' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('63','2017','short','Differentiable Scheduled Sampling for Credit Assignment','We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)--a well-known technique for correcting exposure bias--we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('64','2017','long','Using Global Constraints and Reranking to Improve Cognates Detection','Global constraints and reranking have not been used in cognates detection research to date. We propose methods for using global constraints by performing rescoring of the score matrices produced by state of the art cognates detection systems. Using global constraints to perform rescoring is complementary to state of the art methods for performing cognates detection and results in significant performance improvements beyond current state of the art performance on publicly available datasets with different language pairs and various conditions such as different levels of baseline state of the art performance and different data size conditions, including with more realistic large data size conditions than have been evaluated with in the past.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('65','2017','long','Selective Encoding for Abstractive Sentence Summarization','We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('66','2017','long','Robust Incremental Neural Semantic Graph Parsing','Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('67','2017','long','Predicting Native Language from Gaze','A fundamental question in language learning concerns the role of a speaker''s first language in second language acquisition. We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text. Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented framework complements production studies and offers new ground for advancing research on multilingualism.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('68','2017','long','Abstract Syntax Networks for Code Generation and Semantic Parsing','Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('69','2017','long','Adversarial Multi-Criteria Learning for Chinese Word Segmentation','Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('70','2017','short','Fine-Grained Entity Typing with High-Multiplicity Assignments','As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('71','2017','short','Automatic Compositor Attribution in the First Folio of Shakespeare','Compositor attribution, the clustering of pages in a historical printed document by the individual who set the type, is a bibliographic task that relies on analysis of orthographic variation and inspection of visual details of the printed page. In this paper, we introduce a novel unsupervised model that jointly describes the textual and visual features needed to distinguish compositors. Applied to images of Shakespeare''s First Folio, our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87%, even on text that is the output of OCR.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('72','2017','long','Topically Driven Neural Language Model','Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('73','2017','long','Riemannian Optimization for Skip-Gram Negative Sampling','Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in "word2vec" software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('74','2017','long','Multimodal Word Distributions','Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('75','2017','long','Neural Word Segmentation with Rich Pretraining','Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('76','2017','short','A Conditional Variational Framework for Dialog Generation','Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('77','2017','long','Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings','We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('78','2017','short','Data Augmentation for Low-Resource Neural Machine Translation','The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('79','2017','long','Deep Neural Machine Translation with Linear Associative Unit','Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with their capability in modeling complex functions and capturing complex linguistic structures. However NMT systems with deep architecture in their encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often make the optimization much more difficult. To address this problem we propose novel linear associative units (LAU) to reduce the gradient propagation length inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs utilizes linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time direction. The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('80','2017','long','Probabilistic Typology: Deep Generative Models of Vowel Inventories','Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('81','2017','long','Cross-lingual Distillation for Text Classification','Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('82','2017','long','Flexible and Creative Chinese Poetry Generation Using Neural Memory','It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('83','2017','long','Joint Modeling of Content and Discourse Relations in Dialogues','We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform state-of-the-art approaches for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members'' understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('84','2017','long','Universal Dependencies Parsing for Colloquial Singaporean English','Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('85','2017','long','Verb Physics: Relative Physical Knowledge of Actions and Objects','Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., "My house is bigger than me." However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, "Tyler entered his house" implies that his house is bigger than Tyler.   In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('86','2017','short','A Deep Network with Visual Text Composition Behavior','While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('87','2017','long','A Nested Attention Neural Hybrid Model for Grammatical Error Correction','Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information,and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('88','2016','long','Composing Distributed Representations of Relational Patterns','Learning distributed representations for relation instances is a central technique in downstream NLP applications. In order to address semantic modeling of relational patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset. In addition, we conduct a comparative study of different encoders including additive composition, RNN, LSTM, and GRU for composing distributed representations of relational patterns. We also present Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in the relation classification task.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('89','2017','short','A Generative Parser with a Discriminative Recognition Algorithm','Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('90','2017','short','Group Sparse CNNs for Question Classification with Answer Sets','Question classification is an important task with wide applications. However, traditional techniques treat questions as general sentences, ignoring the corresponding answer data. In order to consider answer information into question modeling, we first introduce novel group sparse autoencoders which refine question representation by utilizing group information in the answer set. We then propose novel group sparse CNNs which naturally learn question representation with respect to their answers by implanting group sparse autoencoders into traditional CNNs. The proposed model significantly outperform strong baselines on four datasets.')

Insert into PUBLICATIONS (PUBLICATIONID,YEAR,TYPE,TITLE,SUMMARY) values ('91','2016','long','One for All: Towards Language Independent Named Entity Linking','Entity linking (EL) is the task of disambiguating mentions in text by associating them with entries in a predefined database of mentions (persons, organizations, etc). Most previous EL research has focused mainly on one language, English, with less attention being paid to other languages, such as Spanish or Chinese. In this paper, we introduce LIEL, a Language Independent Entity Linking system, which provides an EL framework which, once trained on one language, works remarkably well on a number of different languages without change. LIEL makes a joint global prediction over the entire document, employing a discriminative reranking framework with many domain and language-independent feature functions. Experiments on numerous benchmark datasets, show that the proposed system, once trained on one language, English, outperforms several state-of-the-art systems in English (by 4 points) and the trained model also works very well on Spanish (14 points better than a competitor system), demonstrating the viability of the approach.')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('1','Hannah Rashkin')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('1','Sameer Singh')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('1','Yejin Choi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('10','Abhinav Rastogi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('10','Christopher D. Manning')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('10','Christopher Potts')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('10','Jon Gauthier')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('10','Raghav Gupta')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('10','Samuel R. Bowman')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('11','Alessandro Presta')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('11','Aliaksei Severyn')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('11','Chris Alberti')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('11','Daniel Andor')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('11','David Weiss')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('11','Kuzman Ganchev')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('11','Michael Collins')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('11','Slav Petrov')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('12','Eduard Hovy')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('12','Eric Xing')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('12','Xuezhe Ma')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('12','Zhengzhong Liu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('12','Zhiting Hu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('13','Hang Li')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('13','Jiatao Gu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('13','Victor O.K. Li')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('13','Zhengdong Lu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('14','Andrew Senior')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('14','Edward Grefenstette')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('14','Fumin Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('14','Karl Moritz Hermann')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('14','Phil Blunsom')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('14','Tom Ko?isk')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('14','Wang Ling')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('15','Jianpeng Cheng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('15','Mirella Lapata')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('16','Ardavan Saeedi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('16','Karthik Narasimhan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('16','Kayhan Batmanghelich')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('16','Sam Gershman')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('17','Karl Pichotta')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('17','Raymond J. Mooney')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('18','Boris Katz')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('18','Carolyn Spadine')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('18','Jessica Kenney')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('18','Jing Xian Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('18','Keiko Sophie Mori')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('18','Lucia Lam')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('18','Sebastian Garza')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('18','Yevgeni Berzak')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('19','Aurelien Waite')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('19','Bill Byrne')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('19','Eva Hasler')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('19','Felix Stahlberg')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('2','Alexandra Birch')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('2','Barry Haddow')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('2','Rico Sennrich')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('20','Michael Roth')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('20','Mirella Lapata')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('21','Bhuwan Dhingra')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('21','Hanxiao Liu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('21','Ruslan Salakhutdinov')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('21','William Cohen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('21','Zhilin Yang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('22','Christopher D. Manning')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('22','Percy Liang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('22','Sida I. Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('23','Jessica Ficler')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('23','Yoav Goldberg')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('24','Chaitanya Shivade')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('24','Preethi Raghavan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('24','Siddharth Patwardhan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('25','Percy Liang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('25','Robin Jia')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('26','Blaise Thomson')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('26','Diarmuid  Saghdha')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('26','Nikola Mrki')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('26','Steve Young')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('26','Tsung-Hsien Wen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('27','Dimitrios Alikaniotis')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('27','Helen Yannakoudakis')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('27','Marek Rei')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('28','Alexey Borisov')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('28','Maarten de Rijke')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('28','Tom Kenter')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('29','Dan Roth')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('29','Haoruo Peng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('3','Ji He')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('3','Jianfeng Gao')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('3','Jianshu Chen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('3','Li Deng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('3','Lihong Li')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('3','Mari Ostendorf')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('3','Xiaodong He')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('30','Angeliki Lazaridou')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('30','Denis Paperno')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('30','Gemma Boleda')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('30','Germn Kruszewski')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('30','Marco Baroni')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('30','Ngoc Quan Pham')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('30','Raffaella Bernardi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('30','Raquel Fernandez')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('31','James Cross')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('31','Liang Huang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('32','Panupong Pasupat')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('32','Percy Liang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('33','Hinrich Schtze')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('33','Yadollah Yaghoobzadeh')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('34','Eric Xing')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('34','Hao Zhang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('34','Mrinmaya Sachan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('34','Yuntian Deng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('34','Zhicheng Yan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('34','Zhiting Hu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('35','Ella Rabinovich')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('35','Noam Ordan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('35','Sergiu Nisioi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('35','Shuly Wintner')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('36','Diana Inkpen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('36','Hui Jiang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('36','Qian Chen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('36','Si Wei')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('36','Xiaodan Zhu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('36','Zhen-Hua Ling')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('37','Ge Li')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('37','Lili Mou')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('37','Yan Xu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('37','Yunchuan Chen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('37','Zhi Jin')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('38','Roee Aharoni')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('38','Yoav Goldberg')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('39','Abhijit Mishra')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('39','Diptesh Kanojia')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('39','Kuntal Dey')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('39','Pushpak Bhattacharyya')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('39','Seema Nagar')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('4','Alexandra Birch')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('4','Barry Haddow')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('4','Rico Sennrich')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('40','Daniel Gildea')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('40','Linfeng Song')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('40','Xiaochang Peng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('40','Yue Zhang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('40','Zhiguo Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('41','Noah A. Smith')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('41','Yangfeng Ji')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('42','Pramod Viswanath')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('42','Suma Bhat')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('42','Tarek Sakakini')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('43','Akiko Eriguchi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('43','Kyunghyun Cho')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('43','Yoshimasa Tsuruoka')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('44','Adam Fisch')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('44','Antoine Bordes')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('44','Danqi Chen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('44','Jason Weston')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('45','Hinrich Schtze')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('45','Katharina Kann')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('45','Ryan Cotterell')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('46','Anders Sgaard')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('46','Isabelle Augenstein')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('47','Ari Rappoport')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('47','Daniel Hershcovich')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('47','Omri Abend')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('48','Graham Neubig')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('48','Pengcheng Yin')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('49','Fahim Dalvi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('49','Hassan Sajjad')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('49','James Glass')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('49','Nadir Durrani')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('49','Yonatan Belinkov')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('5','Hua Wu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('5','Maosong Sun')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('5','Shiqi Shen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('5','Wei He')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('5','Yang Liu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('5','Yong Cheng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('5','Zhongjun He')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('50','Chieh Lo')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('50','Frederick Liu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('50','Graham Neubig')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('50','Han Lu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('51','Eduard Hovy')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('51','Qizhe Xie')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('51','Xuezhe Ma')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('51','Zihang Dai')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('52','Iryna Gurevych')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('52','Johannes Daxenberger')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('52','Steffen Eger')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('53','Bing Xiang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('53','Bowen Zhou')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('53','Cicero dos Santos')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('53','Kazi Saidul Hasan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('53','Mo Yu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('53','Wenpeng Yin')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('54','Artem Sokolov')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('54','Julia Kreutzer')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('54','Stefan Riezler')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('55','David Weir')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('55','Jeremy Reffin')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('55','Julie Weeds')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('55','Thomas Kober')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('56','Michael Strube')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('56','Nafise Sadat Moosavi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('57','Claire Cardie')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('57','Joonsuk Park')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('57','Vlad Niculae')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('58','Adams Wei Yu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('58','Hongrae Lee')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('58','Quoc Le')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('59','Daqing He')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('59','Rui Meng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('59','Sanqiang Zhao')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('59','Shuguang Han')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('6','David Grangier')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('6','Michael Auli')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('6','Wenlin Chen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('60','Graham Neubig')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('60','Koichiro Yoshino')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('60','Philip Arthur')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('60','Satoshi Nakamura')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('60','Yusuke Oda')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('61','Christopher D. Manning')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('61','Percy Liang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('61','Sam Ginn')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('61','Sida I. Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('62','Anca Dragan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('62','Dan Klein')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('62','Jacob Andreas')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('63','Chris Dyer')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('63','Kartik Goyal')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('63','Taylor Berg-Kirkpatrick')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('64','Benjamin Strauss')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('64','Michael Bloodgood')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('65','Furu Wei')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('65','Ming Zhou')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('65','Nan Yang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('65','Qingyu Zhou')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('66','Jan Buys')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('66','Phil Blunsom')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('67','Boris Katz')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('67','Chie Nakamura')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('67','Suzanne Flynn')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('67','Yevgeni Berzak')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('68','Dan Klein')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('68','Maxim Rabinovich')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('68','Mitchell Stern')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('69','Xinchi Chen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('69','Xipeng Qiu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('69','Xuanjing Huang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('69','Zhan Shi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('7','Julian Hitschler')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('7','Shigehiko Schamoni')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('7','Stefan Riezler')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('70','Dan Klein')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('70','Maxim Rabinovich')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('71','Dan Garrette')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('71','Hannah Alpert-Abrams')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('71','Maria Ryskina')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('71','Taylor Berg-Kirkpatrick')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('72','Jey Han Lau')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('72','Timothy Baldwin')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('72','Trevor Cohn')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('73','Alexander Fonarev')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('73','Gleb Gusev')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('73','Ivan Oseledets')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('73','Oleksii Hrinchuk')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('73','Pavel Serdyukov')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('74','Andrew Wilson')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('74','Ben Athiwaratkun')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('75','Fei Dong')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('75','Jie Yang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('75','Yue Zhang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('76','Akiko Aizawa')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('76','Hui Su')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('76','Shuzi Niu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('76','Wenjie Li')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('76','Xiaoyu Shen')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('76','Yanran Li')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('77','John Wieting')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('77','Kevin Gimpel')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('78','Arianna Bisazza')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('78','Christof Monz')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('78','Marzieh Fadaee')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('79','Jie Zhou')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('79','Mingxuan Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('79','Qun Liu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('79','Zhengdong Lu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('8','Eric Xing')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('8','Kumar Dubey')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('8','Mrinmaya Sachan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('80','Jason Eisner')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('80','Ryan Cotterell')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('81','Ruochen Xu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('81','Yiming Yang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('82','Andi Zhang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('82','Andrew Abel')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('82','Dong Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('82','Jiyuan Zhang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('82','Shiyue Zhang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('82','Yang Feng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('82','Yang Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('83','Joseph Kim')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('83','Julie Shah')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('83','Kechen Qin')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('83','Lu Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('84','GuangYong Leonard Chan')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('84','Hai Leong Chieu')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('84','Hongmin Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('84','Jie Yang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('84','Yue Zhang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('85','Maxwell Forbes')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('85','Yejin Choi')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('86','Hongyu GUO')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('87','Jianfeng Gao')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('87','Jianshu Ji')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('87','Kristina Toutanova')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('87','Qinlong Wang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('87','Steven Truong')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('87','Yongen Gong')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('88','Kentaro Inui')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('88','Naoaki Okazaki')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('88','Sho Takase')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('89','Adam Lopez')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('89','Jianpeng Cheng')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('89','Mirella Lapata')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('9','Jan Kleindienst')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('9','Martin Schmid')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('9','Ondej Bajgar')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('9','Rudolf Kadlec')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('90','Bing Xiang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('90','Bowen Zhou')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('90','Liang Huang')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('90','Mingbo Ma')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('91','Avirup Sil')

Insert into AUTHORS (PUBLICATIONID,AUTHOR) values ('91','Radu Florian')

Database is ready for search.

Menu:
1. View table contents
2. Search by PUBLICATIONID
3. Search by one or more attributes
4. Exit
Enter your choice: 1
View PUBLICATIONS (Yes/No): yes
View AUTHORS (Yes/No): yes

--- PUBLICATIONS ---
PUBLICATIONID: 16, YEAR: 2016, TYPE: short, TITLE: Nonparametric Spherical Topic Modeling with Word Embeddings, SUMMARY: Traditional topic models do not account for semantic regularities in language. Recent distributional representations of words exhibit semantic consistency over directional metrics such as cosine similarity. However, neither categorical nor Gaussian observational distributions used in existing topic models are appropriate to leverage such correlations. In this paper, we propose to use the von Mises-Fisher distribution to model the density of words over a unit sphere. Such a representation is well-suited for directional data. We use a Hierarchical Dirichlet Process for our base topic model and propose an efficient inference algorithm based on Stochastic Variational Inference. This model enables us to naturally exploit the semantic structures of word embeddings while flexibly discovering the number of topics. Experiments demonstrate that our method outperforms competitive approaches in terms of topic coherence on two different text corpora while offering efficient inference.
PUBLICATIONID: 17, YEAR: 2016, TYPE: long, TITLE: Using Sentence-Level LSTM Language Models for Script Inference, SUMMARY: There is a small but growing body of research on statistical scripts, models of event sequences that allow probabilistic inference of implicit events from documents. These systems operate on structured verb-argument events produced by an NLP pipeline. We compare these systems with recent Recurrent Neural Net models that directly operate on raw tokens to predict sentences, finding the latter to be roughly comparable to the former in terms of predicting missing events in documents.
PUBLICATIONID: 18, YEAR: 2016, TYPE: long, TITLE: Universal Dependencies for Learner English, SUMMARY: We introduce the Treebank of Learner English (TLE), the first publicly available syntactic treebank for English as a Second Language (ESL). The TLE provides manually annotated POS tags and Universal Dependency (UD) trees for 5,124 sentences from the Cambridge First Certificate in English (FCE) corpus. The UD annotations are tied to a pre-existing error annotation of the FCE, whereby full syntactic analyses are provided for both the original and error corrected versions of each sentence. Further on, we delineate ESL annotation guidelines that allow for consistent syntactic treatment of ungrammatical English. Finally, we benchmark POS tagging and dependency parsing performance on the TLE dataset and measure the effect of grammatical errors on parsing accuracy. We envision the treebank to support a wide range of linguistic and computational research on second language acquisition as well as automatic processing of ungrammatical language. The treebank is available at universaldependencies.org. The annotation manual used in this project and a graphical query engine are available at esltreebank.org.
PUBLICATIONID: 19, YEAR: 2016, TYPE: short, TITLE: Syntactically Guided Neural Machine Translation, SUMMARY: We investigate the use of hierarchical phrase-based SMT lattices in end-to-end neural machine translation (NMT). Weight pushing transforms the Hiero scores for complete translation hypotheses, with the full translation grammar score and full n-gram language model score, into posteriors compatible with NMT predictive probabilities. With a slightly modified NMT beam-search decoder we find gains over both Hiero and NMT decoding alone, with practical advantages in extending NMT to very large input and output vocabularies.
PUBLICATIONID: 20, YEAR: 2016, TYPE: long, TITLE: Neural Semantic Role Labeling with Dependency Path Embeddings, SUMMARY: This paper introduces a novel model for semantic role labeling that makes use of neural sequence modeling techniques. Our approach is motivated by the observation that complex syntactic structures and related phenomena, such as nested subordinations and nominal predicates, are not handled well by existing models. Our model treats such instances as sub-sequences of lexicalized dependency paths and learns suitable embedding representations. We experimentally demonstrate that such embeddings can improve results over previous state-of-the-art semantic role labelers, and showcase qualitative improvements obtained by our method.
PUBLICATIONID: 21, YEAR: 2017, TYPE: long, TITLE: Gated-Attention Readers for Text Comprehension, SUMMARY: In this paper we study the problem of answering cloze-style questions over documents. Our model, the Gated-Attention (GA) Reader, integrates a multi-hop architecture with a novel attention mechanism, which is based on multiplicative interactions between the query embedding and the intermediate states of a recurrent neural network document reader. This enables the reader to build query-specific representations of tokens in the document for accurate answer selection. The GA Reader obtains state-of-the-art results on three benchmarks for this task--the CNN and Daily Mail news stories and the Who Did What dataset. The effectiveness of multiplicative interaction is demonstrated by an ablation study, and by comparing to alternative compositional operators for implementing the gated-attention. The code is available at https://github.com/bdhingra/ga-reader.
PUBLICATIONID: 22, YEAR: 2016, TYPE: long, TITLE: Learning Language Games through Interaction, SUMMARY: We introduce a new language learning setting relevant to building adaptive natural language interfaces. It is inspired by Wittgenstein's language games: a human wishes to accomplish some task (e.g., achieving a certain configuration of blocks), but can only communicate with a computer, who performs the actual actions (e.g., removing all red blocks). The computer initially knows nothing about language and therefore must learn it from scratch through interaction, while the human adapts to the computer's capabilities. We created a game in a blocks world and collected interactions from 100 people playing it. First, we analyze the humans' strategies, showing that using compositionality and avoiding synonyms correlates positively with task performance. Second, we compare computer strategies, showing how to quickly learn a semantic parsing model from scratch, and that modeling pragmatics further accelerates learning for successful players.
PUBLICATIONID: 23, YEAR: 2016, TYPE: long, TITLE: Coordination Annotation Extension in the Penn Tree Bank, SUMMARY: Coordination is an important and common syntactic construction which is not handled well by state of the art parsers. Coordinations in the Penn Treebank are missing internal structure in many cases, do not include explicit marking of the conjuncts and contain various errors and inconsistencies. In this work, we initiated manual annotation process for solving these issues. We identify the different elements in a coordination phrase and label each element with its function. We add phrase boundaries when these are missing, unify inconsistencies, and fix errors. The outcome is an extension of the PTB that includes consistent and detailed structures for coordinations. We make the coordination annotation publicly available, in hope that they will facilitate further research into coordination disambiguation.
PUBLICATIONID: 24, YEAR: 2016, TYPE: long, TITLE: Addressing Limited Data for Textual Entailment Across Domains, SUMMARY: We seek to address the lack of labeled data (and high cost of annotation) for textual entailment in some domains. To that end, we first create (for experimental purposes) an entailment dataset for the clinical domain, and a highly competitive supervised entailment system, ENT, that is effective (out of the box) on two domains. We then explore self-training and active learning strategies to address the lack of labeled data. With self-training, we successfully exploit unlabeled data to improve over ENT by 15% F-score on the newswire domain, and 13% F-score on clinical data. On the other hand, our active learning experiments demonstrate that we can match (and even beat) ENT using only 6.6% of the training data in the clinical domain, and only 5.8% of the training data in the newswire domain.
PUBLICATIONID: 25, YEAR: 2016, TYPE: long, TITLE: Data Recombination for Neural Semantic Parsing, SUMMARY: Modeling crisp logical regularities is crucial in semantic parsing, making it difficult for neural models with no task-specific prior knowledge to achieve good results. In this paper, we introduce data recombination, a novel framework for injecting such prior knowledge into a model. From the training data, we induce a high-precision synchronous context-free grammar, which captures important conditional independence properties commonly found in semantic parsing. We then train a sequence-to-sequence recurrent network (RNN) model with a novel attention-based copying mechanism on datapoints sampled from this grammar, thereby teaching the model about these structural properties. Data recombination improves the accuracy of our RNN model on three semantic parsing datasets, leading to new state-of-the-art performance on the standard GeoQuery dataset for models with comparable supervision.
PUBLICATIONID: 26, YEAR: 2017, TYPE: long, TITLE: Neural Belief Tracker: Data-Driven Dialogue State Tracking, SUMMARY: One of the core components of modern spoken dialogue systems is the belief tracker, which estimates the user's goal at every step of the dialogue. However, most current approaches have difficulty scaling to larger, more complex dialogue domains. This is due to their dependency on either: a) Spoken Language Understanding models that require large amounts of annotated training data; or b) hand-crafted lexicons for capturing some of the linguistic variation in users' language. We propose a novel Neural Belief Tracking (NBT) framework which overcomes these problems by building on recent advances in representation learning. NBT models reason over pre-trained word vectors, learning to compose them into distributed representations of user utterances and dialogue context. Our evaluation on two datasets shows that this approach surpasses past limitations, matching the performance of state-of-the-art models which rely on hand-crafted semantic lexicons and outperforming them when such lexicons are not provided.
PUBLICATIONID: 27, YEAR: 2016, TYPE: long, TITLE: Automatic Text Scoring Using Neural Networks, SUMMARY: Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.
PUBLICATIONID: 28, YEAR: 2016, TYPE: long, TITLE: Siamese CBOW: Optimizing Word Embeddings for Sentence Representations, SUMMARY: We present the Siamese Continuous Bag of Words (Siamese CBOW) model, a neural network for efficient estimation of high-quality sentence embeddings. Averaging the embeddings of words in a sentence has proven to be a surprisingly successful and efficient way of obtaining sentence embeddings. However, word embeddings trained with the methods currently available are not optimized for the task of sentence representation, and, thus, likely to be suboptimal. Siamese CBOW handles this problem by training word embeddings directly for the purpose of being averaged. The underlying neural network learns word embeddings by predicting, from a sentence representation, its surrounding sentences. We show the robustness of the Siamese CBOW model by evaluating it on 20 datasets stemming from a wide variety of sources.
PUBLICATIONID: 29, YEAR: 2016, TYPE: long, TITLE: Two Discourse Driven Language Models for Semantics, SUMMARY: Natural language understanding often requires deep semantic knowledge. Expanding on previous proposals, we suggest that some important aspects of semantic knowledge can be modeled as a language model if done at an appropriate level of abstraction. We develop two distinct models that capture semantic frame chains and discourse information while abstracting over the specific mentions of predicates and entities. For each model, we investigate four implementations: a "standard" N-gram language model and three discriminatively trained "neural" language models that generate embeddings for semantic frames. The quality of the semantic language models (SemLM) is evaluated both intrinsically, using perplexity and a narrative cloze test and extrinsically - we show that our SemLM helps improve performance on semantic natural language processing tasks such as co-reference resolution and discourse parsing.
PUBLICATIONID: 30, YEAR: 2016, TYPE: long, TITLE: The LAMBADA dataset: Word prediction requiring a broad discourse context, SUMMARY: We introduce LAMBADA, a dataset to evaluate the capabilities of computational models for text understanding by means of a word prediction task. LAMBADA is a collection of narrative passages sharing the characteristic that human subjects are able to guess their last word if they are exposed to the whole passage, but not if they only see the last sentence preceding the target word. To succeed on LAMBADA, computational models cannot simply rely on local context, but must be able to keep track of information in the broader discourse. We show that LAMBADA exemplifies a wide range of linguistic phenomena, and that none of several state-of-the-art language models reaches accuracy above 1% on this novel benchmark. We thus propose LAMBADA as a challenging test set, meant to encourage the development of new models capable of genuine understanding of broad context in natural language text.
PUBLICATIONID: 31, YEAR: 2016, TYPE: short, TITLE: Incremental Parsing with Minimal Features Using Bi-Directional LSTM, SUMMARY: Recently, neural network approaches for parsing have largely automated the combination of individual features, but still rely on (often a larger number of) atomic features created from human linguistic intuition, and potentially omitting important global context. To further reduce feature engineering to the bare minimum, we use bi-directional LSTM sentence representations to model a parser state with only three sentence positions, which automatically identifies important aspects of the entire sentence. This model achieves state-of-the-art results among greedy dependency parsers for English. We also introduce a novel transition system for constituency parsing which does not require binarization, and together with the above architecture, achieves state-of-the-art results among greedy parsers for both English and Chinese.
PUBLICATIONID: 32, YEAR: 2016, TYPE: long, TITLE: Inferring Logical Forms From Denotations, SUMMARY: A core problem in learning semantic parsers from denotations is picking out consistent logical forms--those that yield the correct denotation--from a combinatorially large space. To control the search space, previous work relied on restricted set of rules, which limits expressivity. In this paper, we consider a much more expressive class of logical forms, and show how to use dynamic programming to efficiently represent the complete set of consistent logical forms. Expressivity also introduces many more spurious logical forms which are consistent with the correct denotation but do not represent the meaning of the utterance. To address this, we generate fictitious worlds and use crowdsourced denotations on these worlds to filter out spurious logical forms. On the WikiTableQuestions dataset, we increase the coverage of answerable questions from 53.5% to 76%, and the additional crowdsourced supervision lets us rule out 92.1% of spurious logical forms.
PUBLICATIONID: 33, YEAR: 2016, TYPE: long, TITLE: Intrinsic Subspace Evaluation of Word Embedding Representations, SUMMARY: We introduce a new methodology for intrinsic evaluation of word representations. Specifically, we identify four fundamental criteria based on the characteristics of natural language that pose difficulties to NLP systems; and develop tests that directly show whether or not representations contain the subspaces necessary to satisfy these criteria. Current intrinsic evaluations are mostly based on the overall similarity or full-space similarity of words and thus view vector representations as points. We show the limits of these point-based intrinsic evaluations. We apply our evaluation methodology to the comparison of a count vector model and several neural network models and demonstrate important properties of these models.
PUBLICATIONID: 34, YEAR: 2016, TYPE: long, TITLE: Learning Concept Taxonomies from Multi-modal Data, SUMMARY: We study the problem of automatically building hypernym taxonomies from textual and visual data. Previous works in taxonomy induction generally ignore the increasingly prominent visual data, which encode important perceptual semantics. Instead, we propose a probabilistic model for taxonomy induction by jointly leveraging text and images. To avoid hand-crafted feature engineering, we design end-to-end features based on distributed representations of images and words. The model is discriminatively trained given a small set of existing ontologies and is capable of building full taxonomies from scratch for a collection of unseen conceptual label items with associated images. We evaluate our model and features on the WordNet hierarchies, where our system outperforms previous approaches by a large gap.
PUBLICATIONID: 35, YEAR: 2016, TYPE: long, TITLE: On the Similarities Between Native, Non-native and Translated Texts, SUMMARY: We present a computational analysis of three language varieties: native, advanced non-native, and translation. Our goal is to investigate the similarities and differences between non-native language productions and translations, contrasting both with native language. Using a collection of computational methods we establish three main results: (1) the three types of texts are easily distinguishable; (2) non-native language and translations are closer to each other than each of them is to native language; and (3) some of these characteristics depend on the source or native language, while others do not, reflecting, perhaps, unified principles that similarly affect translations and non-native language.
PUBLICATIONID: 36, YEAR: 2017, TYPE: long, TITLE: Enhanced LSTM for Natural Language Inference, SUMMARY: Reasoning and inference are central to human and artificial intelligence. Modeling inference in human language is very challenging. With the availability of large annotated data (Bowman et al., 2015), it has recently become feasible to train neural network based inference models, which have shown to be very effective. In this paper, we present a new state-of-the-art result, achieving the accuracy of 88.6% on the Stanford Natural Language Inference Dataset. Unlike the previous top models that use very complicated network architectures, we first demonstrate that carefully designing sequential inference models based on chain LSTMs can outperform all previous models. Based on this, we further show that by explicitly considering recursive architectures in both local inference modeling and inference composition, we achieve additional improvement. Particularly, incorporating syntactic parsing information contributes to our best result---it further improves the performance even when added to the already very strong model.
PUBLICATIONID: 37, YEAR: 2016, TYPE: long, TITLE: Compressing Neural Language Models by Sparse Word Representations, SUMMARY: Neural networks are among the state-of-the-art techniques for language modeling. Existing neural language models typically map discrete words to distributed, dense vector representations. After information processing of the preceding context words by hidden layers, an output layer estimates the probability of the next word. Such approaches are time- and memory-intensive because of the large numbers of parameters for word embeddings and the output layer. In this paper, we propose to compress neural language models by sparse word representations. In the experiments, the number of parameters in our model increases very slowly with the growth of the vocabulary size, which is almost imperceptible. Moreover, our approach not only reduces the parameter space to a large extent, but also improves the performance in terms of the perplexity measure.
PUBLICATIONID: 1, YEAR: 2016, TYPE: long, TITLE: Connotation Frames: A Data-Driven Investigation, SUMMARY: Through a particular choice of a predicate (e.g., "x violated y"), a writer can subtly connote a range of implied sentiments and presupposed facts about the entities x and y: (1) writer's perspective: projecting x as an "antagonist"and y as a "victim", (2) entities' perspective: y probably dislikes x, (3) effect: something bad happened to y, (4) value: y is something valuable, and (5) mental state: y is distressed by the event. We introduce connotation frames as a representation formalism to organize these rich dimensions of connotation using typed relations. First, we investigate the feasibility of obtaining connotative labels through crowdsourcing experiments. We then present models for predicting the connotation frames of verb predicates based on their distributional word representations and the interplay between different types of connotative relations. Empirical results confirm that connotation frames can be induced from various data sources that reflect how people use language and give rise to the connotative meanings. We conclude with analytical results that show the potential use of connotation frames for analyzing subtle biases in online news media.
PUBLICATIONID: 2, YEAR: 2016, TYPE: long, TITLE: Neural Machine Translation of Rare Words with Subword Units, SUMMARY: Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character n-gram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English-German and English-Russian by 1.1 and 1.3 BLEU, respectively.
PUBLICATIONID: 3, YEAR: 2016, TYPE: long, TITLE: Deep Reinforcement Learning with a Natural Language Action Space, SUMMARY: This paper introduces a novel architecture for reinforcement learning with deep neural networks designed to handle state and action spaces characterized by natural language, as found in text-based games. Termed a deep reinforcement relevance network (DRRN), the architecture represents action and state spaces with separate embedding vectors, which are combined with an interaction function to approximate the Q-function in reinforcement learning. We evaluate the DRRN on two popular text games, showing superior performance over other deep Q-learning architectures. Experiments with paraphrased action descriptions show that the model is extracting meaning rather than simply memorizing strings of text.
PUBLICATIONID: 4, YEAR: 2016, TYPE: long, TITLE: Improving Neural Machine Translation Models with Monolingual Data, SUMMARY: Neural Machine Translation (NMT) has obtained state-of-the art performance for several language pairs, while only using parallel data for training. Target-side monolingual data plays an important role in boosting fluency for phrase-based statistical machine translation, and we investigate the use of monolingual data for NMT. In contrast to previous work, which combines NMT models with separately trained language models, we note that encoder-decoder NMT architectures already have the capacity to learn the same information as a language model, and we explore strategies to train with monolingual data without changing the neural network architecture. By pairing monolingual training data with an automatic back-translation, we can treat it as additional parallel training data, and we obtain substantial improvements on the WMT 15 task English<->German (+2.8-3.7 BLEU), and for the low-resourced IWSLT 14 task Turkish->English (+2.1-3.4 BLEU), obtaining new state-of-the-art results. We also show that fine-tuning on in-domain monolingual and parallel data gives substantial improvements for the IWSLT 15 task English->German.
PUBLICATIONID: 5, YEAR: 2016, TYPE: long, TITLE: Minimum Risk Training for Neural Machine Translation, SUMMARY: We propose minimum risk training for end-to-end neural machine translation. Unlike conventional maximum likelihood estimation, minimum risk training is capable of optimizing model parameters directly with respect to arbitrary evaluation metrics, which are not necessarily differentiable. Experiments show that our approach achieves significant improvements over maximum likelihood estimation on a state-of-the-art neural machine translation system across various languages pairs. Transparent to architectures, our approach can be applied to more neural networks and potentially benefit more NLP tasks.
PUBLICATIONID: 6, YEAR: 2016, TYPE: long, TITLE: Strategies for Training Large Vocabulary Neural Language Models, SUMMARY: Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.
PUBLICATIONID: 7, YEAR: 2016, TYPE: long, TITLE: Multimodal Pivots for Image Caption Translation, SUMMARY: We present an approach to improve statistical machine translation of image descriptions by multimodal pivots defined in visual space. The key idea is to perform image retrieval over a database of images that are captioned in the target language, and use the captions of the most similar images for crosslingual reranking of translation outputs. Our approach does not depend on the availability of large amounts of in-domain parallel data, but only relies on available large datasets of monolingually captioned images, and on state-of-the-art convolutional neural networks to compute image similarities. Our experimental evaluation shows improvements of 1 BLEU point over strong baselines.
PUBLICATIONID: 8, YEAR: 2016, TYPE: short, TITLE: Science Question Answering using Instructional Materials, SUMMARY: We provide a solution for elementary science test using instructional materials. We posit that there is a hidden structure that explains the correctness of an answer given the question and instructional materials and present a unified max-margin framework that learns to find these hidden structures (given a corpus of question-answer pairs and instructional materials), and uses what it learns to answer novel elementary science questions. Our evaluation shows that our framework outperforms several strong baselines.
PUBLICATIONID: 9, YEAR: 2016, TYPE: long, TITLE: Text Understanding with the Attention Sum Reader Network, SUMMARY: Several large cloze-style context-question-answer datasets have been introduced recently: the CNN and Daily Mail news data and the Children's Book Test. Thanks to the size of these datasets, the associated text comprehension task is well suited for deep-learning techniques that currently seem to outperform all alternative approaches. We present a new, simple model that uses attention to directly pick the answer from the context as opposed to computing the answer using a blended representation of words in the document as is usual in similar models. This makes the model particularly suitable for question-answering problems where the answer is a single word from the document. Ensemble of our models sets new state of the art on all evaluated datasets.
PUBLICATIONID: 10, YEAR: 2016, TYPE: long, TITLE: A Fast Unified Model for Parsing and Sentence Understanding, SUMMARY: Tree-structured neural networks exploit valuable syntactic parse information as they interpret the meanings of sentences. However, they suffer from two key technical problems that make them slow and unwieldy for large-scale NLP tasks: they usually operate on parsed sentences and they do not directly support batched computation. We address these issues by introducing the Stack-augmented Parser-Interpreter Neural Network (SPINN), which combines parsing and interpretation within a single tree-sequence hybrid model by integrating tree-structured sentence interpretation into the linear sequential structure of a shift-reduce parser. Our model supports batched computation for a speedup of up to 25 times over other tree-structured models, and its integrated parser can operate on unparsed data with little loss in accuracy. We evaluate it on the Stanford NLI entailment task and show that it significantly outperforms other sentence-encoding models.
PUBLICATIONID: 11, YEAR: 2016, TYPE: long, TITLE: Globally Normalized Transition-Based Neural Networks, SUMMARY: We introduce a globally normalized transition-based neural network model that achieves state-of-the-art part-of-speech tagging, dependency parsing and sentence compression results. Our model is a simple feed-forward neural network that operates on a task-specific transition system, yet achieves comparable or better accuracies than recurrent models. We discuss the importance of global as opposed to local normalization: a key insight is that the label bias problem implies that globally normalized models can be strictly more expressive than locally normalized models.
PUBLICATIONID: 12, YEAR: 2016, TYPE: long, TITLE: Harnessing Deep Neural Networks with Logic Rules, SUMMARY: Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems.
PUBLICATIONID: 13, YEAR: 2016, TYPE: long, TITLE: Incorporating Copying Mechanism in Sequence-to-Sequence Learning, SUMMARY: We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.
PUBLICATIONID: 14, YEAR: 2016, TYPE: long, TITLE: Latent Predictor Networks for Code Generation, SUMMARY: Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks.
PUBLICATIONID: 15, YEAR: 2016, TYPE: long, TITLE: Neural Summarization by Extracting Sentences and Words, SUMMARY: Traditional approaches to extractive summarization rely heavily on human-engineered features. In this work we propose a data-driven approach based on neural networks and continuous sentence features. We develop a general framework for single-document summarization composed of a hierarchical document encoder and an attention-based extractor. This architecture allows us to develop different classes of summarization models which can extract sentences or words. We train our models on large scale corpora containing hundreds of thousands of document-summary pairs. Experimental results on two summarization datasets demonstrate that our models obtain results comparable to the state of the art without any access to linguistic annotation.
PUBLICATIONID: 38, YEAR: 2017, TYPE: long, TITLE: Morphological Inflection Generation with Hard Monotonic Attention, SUMMARY: We present a neural model for morphological inflection generation which employs a hard attention mechanism, inspired by the nearly-monotonic alignment commonly found between the characters in a word and the characters in its inflection. We evaluate the model on three previously studied morphological inflection generation datasets and show that it provides state of the art results in various setups compared to previous neural and non-neural approaches. Finally we present an analysis of the continuous representations learned by both the hard and soft attention \cite{bahdanauCB14} models for the task, shedding some light on the features such models extract.
PUBLICATIONID: 39, YEAR: 2016, TYPE: long, TITLE: Harnessing Cognitive Features for Sarcasm Detection, SUMMARY: In this paper, we propose a novel mechanism for enriching the feature vector, for the task of sarcasm detection, with cognitive features extracted from eye-movement patterns of human readers. Sarcasm detection has been a challenging research problem, and its importance for NLP applications such as review summarization, dialog systems and sentiment analysis is well recognized. Sarcasm can often be traced to incongruity that becomes apparent as the full sentence unfolds. This presence of incongruity- implicit or explicit- affects the way readers eyes move through the text. We observe the difference in the behaviour of the eye, while reading sarcastic and non sarcastic sentences. Motivated by his observation, we augment traditional linguistic and stylistic features for sarcasm detection with the cognitive features obtained from readers eye movement data. We perform statistical classification using the enhanced feature set so obtained. The augmented cognitive features improve sarcasm detection by 3.7% (in terms of F-score), over the performance of the best reported system.
PUBLICATIONID: 40, YEAR: 2017, TYPE: short, TITLE: AMR-to-text Generation with Synchronous Node Replacement Grammar, SUMMARY: This paper addresses the task of AMR-to-text generation by leveraging synchronous node replacement grammar. During training, graph-to-string rules are learned using a heuristic extraction algorithm. At test time, a graph transducer is applied to collapse input AMRs and generate output sentences. Evaluated on SemEval-2016 Task 8, our method gives a BLEU score of 25.62, which is the best reported so far.
PUBLICATIONID: 41, YEAR: 2017, TYPE: long, TITLE: Neural Discourse Structure for Text Categorization, SUMMARY: We show that discourse structure, as defined by Rhetorical Structure Theory and provided by an existing discourse parser, benefits text categorization. Our approach uses a recursive neural network and a newly proposed attention mechanism to compute a representation of the text that focuses on salient content, from the perspective of both RST and the task. Experiments consider variants of the approach and illustrate its strengths and weaknesses.
PUBLICATIONID: 42, YEAR: 2017, TYPE: long, TITLE: MORSE: Semantic-ally Drive-n MORpheme SEgment-er, SUMMARY: We present in this paper a novel framework for morpheme segmentation which uses the morpho-syntactic regularities preserved by word representations, in addition to orthographic features, to segment words into morphemes. This framework is the first to consider vocabulary-wide syntactico-semantic information for this task. We also analyze the deficiencies of available benchmarking datasets and introduce our own dataset that was created on the basis of compositionality. We validate our algorithm across datasets and present state-of-the-art results.
PUBLICATIONID: 43, YEAR: 2017, TYPE: short, TITLE: Learning to Parse and Translate Improves Neural Machine Translation, SUMMARY: There has been relatively little attention to incorporating linguistic prior to neural machine translation. Much of the previous work was further constrained to considering linguistic prior on the source side. In this paper, we propose a hybrid model, called NMT+RNNG, that learns to parse and translate by combining the recurrent neural network grammar into the attention-based neural machine translation. Our approach encourages the neural machine translation model to incorporate linguistic prior during training, and lets it translate on its own afterward. Extensive experiments with four language pairs show the effectiveness of the proposed NMT+RNNG.
PUBLICATIONID: 44, YEAR: 2017, TYPE: long, TITLE: Reading Wikipedia to Answer Open-Domain Questions, SUMMARY: This paper proposes to tackle open- domain question answering using Wikipedia as the unique knowledge source: the answer to any factoid question is a text span in a Wikipedia article. This task of machine reading at scale combines the challenges of document retrieval (finding the relevant articles) with that of machine comprehension of text (identifying the answer spans from those articles). Our approach combines a search component based on bigram hashing and TF-IDF matching with a multi-layer recurrent neural network model trained to detect answers in Wikipedia paragraphs. Our experiments on multiple existing QA datasets indicate that (1) both modules are highly competitive with respect to existing counterparts and (2) multitask learning using distant supervision on their combination is an effective complete system on this challenging task.
PUBLICATIONID: 45, YEAR: 2017, TYPE: long, TITLE: One-Shot Neural Cross-Lingual Transfer for Paradigm Completion, SUMMARY: We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.
PUBLICATIONID: 46, YEAR: 2017, TYPE: short, TITLE: Multi-Task Learning of Keyphrase Boundary Classification, SUMMARY: Keyphrase boundary classification (KBC) is the task of detecting keyphrases in scientific articles and labelling them with respect to predefined types. Although important in practice, this task is so far underexplored, partly due to the lack of labelled data. To overcome this, we explore several auxiliary tasks, including semantic super-sense tagging and identification of multi-word expressions, and cast the task as a multi-task learning problem with deep recurrent neural networks. Our multi-task models perform significantly better than previous state of the art approaches on two scientific KBC datasets, particularly for long keyphrases.
PUBLICATIONID: 47, YEAR: 2017, TYPE: long, TITLE: A Transition-Based Directed Acyclic Graph Parser for UCCA, SUMMARY: We present the first parser for UCCA, a cross-linguistically applicable framework for semantic representation, which builds on extensive typological work and supports rapid annotation. UCCA poses a challenge for existing parsing techniques, as it exhibits reentrancy (resulting in DAG structures), discontinuous structures and non-terminal nodes corresponding to complex semantic units. To our knowledge, the conjunction of these formal properties is not supported by any existing parser. Our transition-based parser, which uses a novel transition set and features based on bidirectional LSTMs, has value not just for UCCA parsing: its ability to handle more general graph structures can inform the development of parsers for other semantic DAG structures, and in languages that frequently use discontinuous structures.
PUBLICATIONID: 48, YEAR: 2017, TYPE: long, TITLE: A Syntactic Neural Model for General-Purpose Code Generation, SUMMARY: We consider the problem of parsing natural language descriptions into source code written in a general-purpose programming language like Python. Existing data-driven methods treat this problem as a language generation task without considering the underlying syntax of the target programming language. Informed by previous work in semantic parsing, in this paper we propose a novel neural architecture powered by a grammar model to explicitly capture the target syntax as prior knowledge. Experiments find this an effective way to scale up to generation of complex programs from natural language descriptions, achieving state-of-the-art results that well outperform previous code generation and semantic parsing approaches.
PUBLICATIONID: 49, YEAR: 2017, TYPE: long, TITLE: What do Neural Machine Translation Models Learn about Morphology?, SUMMARY: Neural machine translation (MT) models obtain state-of-the-art performance while maintaining a simple, end-to-end architecture. However, little is known about what these models learn about source and target languages during the training process. In this work, we analyze the representations learned by neural MT models at various levels of granularity and empirically evaluate the quality of the representations for learning morphology through extrinsic part-of-speech and morphological tagging tasks. We conduct a thorough investigation along several parameters: word-based vs. character-based representations, depth of the encoding layer, the identity of the target language, and encoder vs. decoder representations. Our data-driven, quantitative evaluation sheds light on important aspects in the neural MT system and its ability to capture word structure.
PUBLICATIONID: 50, YEAR: 2017, TYPE: long, TITLE: Learning Character-level Compositionality with Visual Features, SUMMARY: Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry semantic content, resulting in embeddings that are coherent in visual space.
PUBLICATIONID: 51, YEAR: 2017, TYPE: long, TITLE: An Interpretable Knowledge Transfer Model for Knowledge Base Completion, SUMMARY: Knowledge bases are important resources for a variety of natural language processing tasks but suffer from incompleteness. We propose a novel embedding model, \emph{ITransF}, to perform knowledge base completion. Equipped with a sparse attention mechanism, ITransF discovers hidden concepts of relations and transfer statistical strength through the sharing of concepts. Moreover, the learned associations between relations and concepts, which are represented by sparse attention vectors, can be interpreted easily. We evaluate ITransF on two benchmark datasets---WN18 and FB15k for knowledge base completion and obtains improvements on both the mean rank and Hits@10 metrics, over all baselines that do not use additional information.
PUBLICATIONID: 52, YEAR: 2017, TYPE: long, TITLE: Neural End-to-End Learning for Computational Argumentation Mining, SUMMARY: We investigate neural techniques for end-to-end computational argumentation mining (AM). We frame AM both as a token-based dependency parsing and as a token-based sequence tagging problem, including a multi-task learning setup. Contrary to models that operate on the argument component level, we find that framing AM as dependency parsing leads to subpar performance results. In contrast, less complex (local) tagging models based on BiLSTMs perform robustly across classification scenarios, being able to catch long-range dependencies inherent to the AM problem. Moreover, we find that jointly learning 'natural' subtasks, in a multi-task learning setup, improves performance.
PUBLICATIONID: 53, YEAR: 2017, TYPE: long, TITLE: Improved Neural Relation Detection for Knowledge Base Question Answering, SUMMARY: Relation detection is a core component for many NLP applications including Knowledge Base Question Answering (KBQA). In this paper, we propose a hierarchical recurrent neural network enhanced by residual learning that detects KB relations given an input question. Our method uses deep residual bidirectional LSTMs to compare questions and relation names via different hierarchies of abstraction. Additionally, we propose a simple KBQA system that integrates entity linking and our proposed relation detector to enable one enhance another. Experimental results evidence that our approach achieves not only outstanding relation detection performance, but more importantly, it helps our KBQA system to achieve state-of-the-art accuracy for both single-relation (SimpleQuestions) and multi-relation (WebQSP) QA benchmarks.
PUBLICATIONID: 54, YEAR: 2017, TYPE: long, TITLE: Bandit Structured Prediction for Neural Sequence-to-Sequence Learning, SUMMARY: Bandit structured prediction describes a stochastic optimization framework where learning is performed from partial feedback. This feedback is received in the form of a task loss evaluation to a predicted output structure, without having access to gold standard structures. We advance this framework by lifting linear bandit learning to neural sequence-to-sequence learning problems using attention-based recurrent neural networks. Furthermore, we show how to incorporate control variates into our learning algorithms for variance reduction and improved generalization. We present an evaluation on a neural machine translation task that shows improvements of up to 5.89 BLEU points for domain adaptation from simulated bandit feedback.
PUBLICATIONID: 55, YEAR: 2017, TYPE: short, TITLE: Improving Semantic Composition with Offset Inference, SUMMARY: Count-based distributional semantic models suffer from sparsity due to unobserved but plausible co-occurrences in any text collection. This problem is amplified for models like Anchored Packed Trees (APTs), that take the grammatical type of a co-occurrence into account. We therefore introduce a novel form of distributional inference that exploits the rich type structure in APTs and infers missing data by the same mechanism that is used for semantic composition.
PUBLICATIONID: 56, YEAR: 2017, TYPE: short, TITLE: Lexical Features in Coreference Resolution: To be Used With Caution, SUMMARY: Lexical features are a major source of information in state-of-the-art coreference resolvers. Lexical features implicitly model some of the linguistic phenomena at a fine granularity level. They are especially useful for representing the context of mentions. In this paper we investigate a drawback of using many lexical features in state-of-the-art coreference resolvers. We show that if coreference resolvers mainly rely on lexical features, they can hardly generalize to unseen domains. Furthermore, we show that the current coreference resolution evaluation is clearly flawed by only evaluating on a specific split of a specific dataset in which there is a notable overlap between the training, development and test sets.
PUBLICATIONID: 57, YEAR: 2017, TYPE: long, TITLE: Argument Mining with Structured SVMs and RNNs, SUMMARY: We propose a novel factor graph model for argument mining, designed for settings in which the argumentative relations in a document do not necessarily form a tree structure. (This is the case in over 20% of the web comments dataset we release.) Our model jointly learns elementary unit type classification and argumentative relation prediction. Moreover, our model supports SVM and RNN parametrizations, can enforce structure constraints (e.g., transitivity), and can express dependencies between adjacent relations and propositions. Our approaches outperform unstructured baselines in both web comments and argumentative essay datasets.
PUBLICATIONID: 58, YEAR: 2017, TYPE: long, TITLE: Learning to Skim Text, SUMMARY: Recurrent Neural Networks are showing much promise in many sub-areas of natural language processing, ranging from document classification to machine translation to automatic question answering. Despite their promise, many recurrent models have to read the whole text word by word, making it slow to handle long documents. For example, it is difficult to use a recurrent network to read a book and answer questions about it. In this paper, we present an approach of reading text while skipping irrelevant information if needed. The underlying model is a recurrent network that learns how far to jump after reading a few words of the input text. We employ a standard policy gradient method to train the model to make discrete jumping decisions. In our benchmarks on four different tasks, including number prediction, sentiment analysis, news article classification and automatic Q and A, our proposed model, a modified LSTM with jumping, is up to 6 times faster than the standard sequential LSTM, while maintaining the same or even better accuracy.
PUBLICATIONID: 59, YEAR: 2017, TYPE: long, TITLE: Deep Keyphrase Generation, SUMMARY: Keyphrase provides highly-summative information that can be effectively used for understanding, organizing and retrieving text content. Though previous studies have provided many workable solutions for automated keyphrase extraction, they commonly divided the to-be-summarized content into multiple text chunks, then ranked and selected the most meaningful ones. These approaches could neither identify keyphrases that do not appear in the text, nor capture the real semantic meaning behind the text. We propose a generative model for keyphrase prediction with an encoder-decoder framework, which can effectively overcome the above drawbacks. We name it as deep keyphrase generation since it attempts to capture the deep semantic meaning of the content with a deep learning method. Empirical analysis on six datasets demonstrates that our proposed model not only achieves a significant performance boost on extracting keyphrases that appear in the source text, but also can generate absent keyphrases based on the semantic meaning of the text. Code and dataset are available at https://github.com/memray/seq2seq-keyphrase.
PUBLICATIONID: 60, YEAR: 2017, TYPE: long, TITLE: Neural Machine Translation via Binary Code Prediction, SUMMARY: In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.
PUBLICATIONID: 61, YEAR: 2017, TYPE: long, TITLE: Naturalizing a Programming Language via Interactive Learning, SUMMARY: Our goal is to create a convenient natural language interface for performing well-specified but complex actions such as analyzing data, manipulating text, and querying databases. However, existing natural language interfaces for such tasks are quite primitive compared to the power one wields with a programming language. To bridge this gap, we start with a core programming language and allow users to "naturalize" the core language incrementally by defining alternative, more natural syntax and increasingly complex concepts in terms of compositions of simpler ones. In a voxel world, we show that a community of users can simultaneously teach a common system a diverse language and use it to build hundreds of complex voxel structures. Over the course of three days, these users went from using only the core language to using the naturalized language in 85.9% of the last 10K utterances.
PUBLICATIONID: 62, YEAR: 2017, TYPE: long, TITLE: Translating Neuralese, SUMMARY: Several approaches have recently been proposed for learning decentralized deep multiagent policies that coordinate via a differentiable communication channel. While these policies are effective for many tasks, interpretation of their induced communication strategies has remained a challenge. Here we propose to interpret agents' messages by translating them. Unlike in typical machine translation problems, we have no parallel data to learn from. Instead we develop a translation model based on the insight that agent messages and natural language strings mean the same thing if they induce the same belief about the world in a listener. We present theoretical guarantees and empirical evidence that our approach preserves both the semantics and pragmatics of messages by ensuring that players communicating through a translation layer do not suffer a substantial loss in reward relative to players with a common language.
PUBLICATIONID: 63, YEAR: 2017, TYPE: short, TITLE: Differentiable Scheduled Sampling for Credit Assignment, SUMMARY: We demonstrate that a continuous relaxation of the argmax operation can be used to create a differentiable approximation to greedy decoding for sequence-to-sequence (seq2seq) models. By incorporating this approximation into the scheduled sampling training procedure (Bengio et al., 2015)--a well-known technique for correcting exposure bias--we introduce a new training objective that is continuous and differentiable everywhere and that can provide informative gradients near points where previous decoding decisions change their value. In addition, by using a related approximation, we demonstrate a similar approach to sampled-based training. Finally, we show that our approach outperforms cross-entropy training and scheduled sampling procedures in two sequence prediction tasks: named entity recognition and machine translation.
PUBLICATIONID: 64, YEAR: 2017, TYPE: long, TITLE: Using Global Constraints and Reranking to Improve Cognates Detection, SUMMARY: Global constraints and reranking have not been used in cognates detection research to date. We propose methods for using global constraints by performing rescoring of the score matrices produced by state of the art cognates detection systems. Using global constraints to perform rescoring is complementary to state of the art methods for performing cognates detection and results in significant performance improvements beyond current state of the art performance on publicly available datasets with different language pairs and various conditions such as different levels of baseline state of the art performance and different data size conditions, including with more realistic large data size conditions than have been evaluated with in the past.
PUBLICATIONID: 65, YEAR: 2017, TYPE: long, TITLE: Selective Encoding for Abstractive Sentence Summarization, SUMMARY: We propose a selective encoding model to extend the sequence-to-sequence framework for abstractive sentence summarization. It consists of a sentence encoder, a selective gate network, and an attention equipped decoder. The sentence encoder and decoder are built with recurrent neural networks. The selective gate network constructs a second level sentence representation by controlling the information flow from encoder to decoder. The second level representation is tailored for sentence summarization task, which leads to better performance. We evaluate our model on the English Gigaword, DUC 2004 and MSR abstractive sentence summarization datasets. The experimental results show that the proposed selective encoding model outperforms the state-of-the-art baseline models.
PUBLICATIONID: 66, YEAR: 2017, TYPE: long, TITLE: Robust Incremental Neural Semantic Graph Parsing, SUMMARY: Parsing sentences to linguistically-expressive semantic representations is a key goal of Natural Language Processing. Yet statistical parsing has focused almost exclusively on bilexical dependencies or domain-specific logical forms. We propose a neural encoder-decoder transition-based parser which is the first full-coverage semantic graph parser for Minimal Recursion Semantics (MRS). The model architecture uses stack-based embedding features, predicting graphs jointly with unlexicalized predicates and their token alignments. Our parser is more accurate than attention-based baselines on MRS, and on an additional Abstract Meaning Representation (AMR) benchmark, and GPU batch processing makes it an order of magnitude faster than a high-precision grammar-based parser. Further, the 86.69% Smatch score of our MRS parser is higher than the upper-bound on AMR parsing, making MRS an attractive choice as a semantic representation.
PUBLICATIONID: 67, YEAR: 2017, TYPE: long, TITLE: Predicting Native Language from Gaze, SUMMARY: A fundamental question in language learning concerns the role of a speaker's first language in second language acquisition. We present a novel methodology for studying this question: analysis of eye-movement patterns in second language reading of free-form text. Using this methodology, we demonstrate for the first time that the native language of English learners can be predicted from their gaze fixations when reading English. We provide analysis of classifier uncertainty and learned features, which indicates that differences in English reading are likely to be rooted in linguistic divergences across native languages. The presented framework complements production studies and offers new ground for advancing research on multilingualism.
PUBLICATIONID: 68, YEAR: 2017, TYPE: long, TITLE: Abstract Syntax Networks for Code Generation and Semantic Parsing, SUMMARY: Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering.
PUBLICATIONID: 69, YEAR: 2017, TYPE: long, TITLE: Adversarial Multi-Criteria Learning for Chinese Word Segmentation, SUMMARY: Different linguistic perspectives causes many diverse segmentation criteria for Chinese word segmentation (CWS). Most existing methods focus on improve the performance for each single criterion. However, it is interesting to exploit these different criteria and mining their common underlying knowledge. In this paper, we propose adversarial multi-criteria learning for CWS by integrating shared knowledge from multiple heterogeneous segmentation criteria. Experiments on eight corpora with heterogeneous segmentation criteria show that the performance of each corpus obtains a significant improvement, compared to single-criterion learning. Source codes of this paper are available on Github.
PUBLICATIONID: 70, YEAR: 2017, TYPE: short, TITLE: Fine-Grained Entity Typing with High-Multiplicity Assignments, SUMMARY: As entity type systems become richer and more fine-grained, we expect the number of types assigned to a given entity to increase. However, most fine-grained typing work has focused on datasets that exhibit a low degree of type multiplicity. In this paper, we consider the high-multiplicity regime inherent in data sources such as Wikipedia that have semi-open type systems. We introduce a set-prediction approach to this problem and show that our model outperforms unstructured baselines on a new Wikipedia-based fine-grained typing corpus.
PUBLICATIONID: 71, YEAR: 2017, TYPE: short, TITLE: Automatic Compositor Attribution in the First Folio of Shakespeare, SUMMARY: Compositor attribution, the clustering of pages in a historical printed document by the individual who set the type, is a bibliographic task that relies on analysis of orthographic variation and inspection of visual details of the printed page. In this paper, we introduce a novel unsupervised model that jointly describes the textual and visual features needed to distinguish compositors. Applied to images of Shakespeare's First Folio, our model predicts attributions that agree with the manual judgements of bibliographers with an accuracy of 87%, even on text that is the output of OCR.
PUBLICATIONID: 72, YEAR: 2017, TYPE: long, TITLE: Topically Driven Neural Language Model, SUMMARY: Language models are typically applied at the sentence level, without access to the broader document context. We present a neural language model that incorporates document context in the form of a topic model-like architecture, thus providing a succinct representation of the broader document context outside of the current sentence. Experiments over a range of datasets demonstrate that our model outperforms a pure sentence-based model in terms of language model perplexity, and leads to topics that are potentially more coherent than those produced by a standard LDA topic model. Our model also has the ability to generate related sentences for a topic, providing another way to interpret topics.
PUBLICATIONID: 73, YEAR: 2017, TYPE: long, TITLE: Riemannian Optimization for Skip-Gram Negative Sampling, SUMMARY: Skip-Gram Negative Sampling (SGNS) word embedding model, well known by its implementation in "word2vec" software, is usually optimized by stochastic gradient descent. However, the optimization of SGNS objective can be viewed as a problem of searching for a good matrix with the low-rank constraint. The most standard way to solve this type of problems is to apply Riemannian optimization framework to optimize the SGNS objective over the manifold of required low-rank matrices. In this paper, we propose an algorithm that optimizes SGNS objective using Riemannian optimization and demonstrates its superiority over popular competitors, such as the original method to train SGNS and SVD over SPPMI matrix.
PUBLICATIONID: 74, YEAR: 2017, TYPE: long, TITLE: Multimodal Word Distributions, SUMMARY: Word embeddings provide point representations of words containing useful semantic information. We introduce multimodal word distributions formed from Gaussian mixtures, for multiple word meanings, entailment, and rich uncertainty information. To learn these distributions, we propose an energy-based max-margin objective. We show that the resulting approach captures uniquely expressive semantic information, and outperforms alternatives, such as word2vec skip-grams, and Gaussian embeddings, on benchmark datasets such as word similarity and entailment.
PUBLICATIONID: 75, YEAR: 2017, TYPE: long, TITLE: Neural Word Segmentation with Rich Pretraining, SUMMARY: Neural word segmentation research has benefited from large-scale raw texts by leveraging them for pretraining character and word embeddings. On the other hand, statistical segmentation research has exploited richer sources of external information, such as punctuation, automatic segmentation and POS. We investigate the effectiveness of a range of external training sources for neural word segmentation by building a modular segmentation model, pretraining the most important submodule using rich external sources. Results show that such pretraining significantly improves the model, leading to accuracies competitive to the best methods on six benchmarks.
PUBLICATIONID: 76, YEAR: 2017, TYPE: short, TITLE: A Conditional Variational Framework for Dialog Generation, SUMMARY: Deep latent variable models have been shown to facilitate the response generation for open-domain dialog systems. However, these latent variables are highly randomized, leading to uncontrollable generated responses. In this paper, we propose a framework allowing conditional response generation based on specific attributes. These attributes can be either manually assigned or automatically detected. Moreover, the dialog states for both speakers are modeled separately in order to reflect personal features. We validate this framework on two different scenarios, where the attribute refers to genericness and sentiment states respectively. The experiment result testified the potential of our model, where meaningful responses can be generated in accordance with the specified attributes.
PUBLICATIONID: 77, YEAR: 2017, TYPE: long, TITLE: Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings, SUMMARY: We consider the problem of learning general-purpose, paraphrastic sentence embeddings, revisiting the setting of Wieting et al. (2016b). While they found LSTM recurrent networks to underperform word averaging, we present several developments that together produce the opposite conclusion. These include training on sentence pairs rather than phrase pairs, averaging states to represent sequences, and regularizing aggressively. These improve LSTMs in both transfer learning and supervised settings. We also introduce a new recurrent architecture, the Gated Recurrent Averaging Network, that is inspired by averaging and LSTMs while outperforming them both. We analyze our learned models, finding evidence of preferences for particular parts of speech and dependency relations.
PUBLICATIONID: 78, YEAR: 2017, TYPE: short, TITLE: Data Augmentation for Low-Resource Neural Machine Translation, SUMMARY: The quality of a Neural Machine Translation system depends substantially on the availability of sizable parallel corpora. For low-resource language pairs this is not the case, resulting in poor translation quality. Inspired by work in computer vision, we propose a novel data augmentation approach that targets low-frequency words by generating new sentence pairs containing rare words in new, synthetically created contexts. Experimental results on simulated low-resource settings show that our method improves translation quality by up to 2.9 BLEU points over the baseline and up to 3.2 BLEU over back-translation.
PUBLICATIONID: 79, YEAR: 2017, TYPE: long, TITLE: Deep Neural Machine Translation with Linear Associative Unit, SUMMARY: Deep Neural Networks (DNNs) have provably enhanced the state-of-the-art Neural Machine Translation (NMT) with their capability in modeling complex functions and capturing complex linguistic structures. However NMT systems with deep architecture in their encoder or decoder RNNs often suffer from severe gradient diffusion due to the non-linear recurrent activations, which often make the optimization much more difficult. To address this problem we propose novel linear associative units (LAU) to reduce the gradient propagation length inside the recurrent unit. Different from conventional approaches (LSTM unit and GRU), LAUs utilizes linear associative connections between input and output of the recurrent unit, which allows unimpeded information flow through both space and time direction. The model is quite simple, but it is surprisingly effective. Our empirical study on Chinese-English translation shows that our model with proper configuration can improve by 11.7 BLEU upon Groundhog and the best reported results in the same setting. On WMT14 English-German task and a larger WMT14 English-French task, our model achieves comparable results with the state-of-the-art.
PUBLICATIONID: 80, YEAR: 2017, TYPE: long, TITLE: Probabilistic Typology: Deep Generative Models of Vowel Inventories, SUMMARY: Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.
PUBLICATIONID: 81, YEAR: 2017, TYPE: long, TITLE: Cross-lingual Distillation for Text Classification, SUMMARY: Cross-lingual text classification(CLTC) is the task of classifying documents written in different languages into the same taxonomy of categories. This paper presents a novel approach to CLTC that builds on model distillation, which adapts and extends a framework originally proposed for model compression. Using soft probabilistic predictions for the documents in a label-rich language as the (induced) supervisory labels in a parallel corpus of documents, we train classifiers successfully for new languages in which labeled training data are not available. An adversarial feature adaptation technique is also applied during the model training to reduce distribution mismatch. We conducted experiments on two benchmark CLTC datasets, treating English as the source language and German, French, Japan and Chinese as the unlabeled target languages. The proposed approach had the advantageous or comparable performance of the other state-of-art methods.
PUBLICATIONID: 82, YEAR: 2017, TYPE: long, TITLE: Flexible and Creative Chinese Poetry Generation Using Neural Memory, SUMMARY: It has been shown that Chinese poems can be successfully generated by sequence-to-sequence neural models, particularly with the attention mechanism. A potential problem of this approach, however, is that neural models can only learn abstract rules, while poem generation is a highly creative process that involves not only rules but also innovations for which pure statistical models are not appropriate in principle. This work proposes a memory-augmented neural model for Chinese poem generation, where the neural model and the augmented memory work together to balance the requirements of linguistic accordance and aesthetic innovation, leading to innovative generations that are still rule-compliant. In addition, it is found that the memory mechanism provides interesting flexibility that can be used to generate poems with different styles.
PUBLICATIONID: 83, YEAR: 2017, TYPE: long, TITLE: Joint Modeling of Content and Discourse Relations in Dialogues, SUMMARY: We present a joint modeling approach to identify salient discussion points in spoken meetings as well as to label the discourse relations between speaker turns. A variation of our model is also discussed when discourse relations are treated as latent variables. Experimental results on two popular meeting corpora show that our joint model can outperform state-of-the-art approaches for both phrase-based content selection and discourse relation prediction tasks. We also evaluate our model on predicting the consistency among team members' understanding of their group decisions. Classifiers trained with features constructed from our model achieve significant better predictive performance than the state-of-the-art.
PUBLICATIONID: 84, YEAR: 2017, TYPE: long, TITLE: Universal Dependencies Parsing for Colloquial Singaporean English, SUMMARY: Singlish can be interesting to the ACL community both linguistically as a major creole based on English, and computationally for information extraction and sentiment analysis of regional social media. We investigate dependency parsing of Singlish by constructing a dependency treebank under the Universal Dependencies scheme, and then training a neural network model by integrating English syntactic knowledge into a state-of-the-art parser trained on the Singlish treebank. Results show that English knowledge can lead to 25% relative error reduction, resulting in a parser of 84.47% accuracies. To the best of our knowledge, we are the first to use neural stacking to improve cross-lingual dependency parsing on low-resource languages. We make both our annotation and parser available for further research.
PUBLICATIONID: 85, YEAR: 2017, TYPE: long, TITLE: Verb Physics: Relative Physical Knowledge of Actions and Objects, SUMMARY: Learning commonsense knowledge from natural language text is nontrivial due to reporting bias: people rarely state the obvious, e.g., "My house is bigger than me." However, while rarely stated explicitly, this trivial everyday knowledge does influence the way people talk about the world, which provides indirect clues to reason about the world. For example, a statement like, "Tyler entered his house" implies that his house is bigger than Tyler.   In this paper, we present an approach to infer relative physical knowledge of actions and objects along five dimensions (e.g., size, weight, and strength) from unstructured natural language text. We frame knowledge acquisition as joint inference over two closely related problems: learning (1) relative physical knowledge of object pairs and (2) physical implications of actions when applied to those object pairs. Empirical results demonstrate that it is possible to extract knowledge of actions and objects from language and that joint inference over different types of knowledge improves performance.
PUBLICATIONID: 86, YEAR: 2017, TYPE: short, TITLE: A Deep Network with Visual Text Composition Behavior, SUMMARY: While natural languages are compositional, how state-of-the-art neural models achieve compositionality is still unclear. We propose a deep network, which not only achieves competitive accuracy for text classification, but also exhibits compositional behavior. That is, while creating hierarchical representations of a piece of text, such as a sentence, the lower layers of the network distribute their layer-specific attention weights to individual words. In contrast, the higher layers compose meaningful phrases and clauses, whose lengths increase as the networks get deeper until fully composing the sentence.
PUBLICATIONID: 87, YEAR: 2017, TYPE: long, TITLE: A Nested Attention Neural Hybrid Model for Grammatical Error Correction, SUMMARY: Grammatical error correction (GEC) systems strive to correct both global errors in word order and usage, and local errors in spelling and inflection. Further developing upon recent work on neural machine translation, we propose a new hybrid neural model with nested attention layers for GEC. Experiments show that the new model can effectively correct errors of both types by incorporating word and character-level information,and that the model significantly outperforms previous neural models for GEC as measured on the standard CoNLL-14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography.
PUBLICATIONID: 88, YEAR: 2016, TYPE: long, TITLE: Composing Distributed Representations of Relational Patterns, SUMMARY: Learning distributed representations for relation instances is a central technique in downstream NLP applications. In order to address semantic modeling of relational patterns, this paper constructs a new dataset that provides multiple similarity ratings for every pair of relational patterns on the existing dataset. In addition, we conduct a comparative study of different encoders including additive composition, RNN, LSTM, and GRU for composing distributed representations of relational patterns. We also present Gated Additive Composition, which is an enhancement of additive composition with the gating mechanism. Experiments show that the new dataset does not only enable detailed analyses of the different encoders, but also provides a gauge to predict successes of distributed representations of relational patterns in the relation classification task.
PUBLICATIONID: 89, YEAR: 2017, TYPE: short, TITLE: A Generative Parser with a Discriminative Recognition Algorithm, SUMMARY: Generative models defining joint distributions over parse trees and sentences are useful for parsing and language modeling, but impose restrictions on the scope of features and are often outperformed by discriminative models. We propose a framework for parsing and language modeling which marries a generative model with a discriminative recognition model in an encoder-decoder setting. We provide interpretations of the framework based on expectation maximization and variational inference, and show that it enables parsing and language modeling within a single implementation. On the English Penn Treen-bank, our framework obtains competitive performance on constituency parsing while matching the state-of-the-art single-model language modeling score.
PUBLICATIONID: 90, YEAR: 2017, TYPE: short, TITLE: Group Sparse CNNs for Question Classification with Answer Sets, SUMMARY: Question classification is an important task with wide applications. However, traditional techniques treat questions as general sentences, ignoring the corresponding answer data. In order to consider answer information into question modeling, we first introduce novel group sparse autoencoders which refine question representation by utilizing group information in the answer set. We then propose novel group sparse CNNs which naturally learn question representation with respect to their answers by implanting group sparse autoencoders into traditional CNNs. The proposed model significantly outperform strong baselines on four datasets.
PUBLICATIONID: 91, YEAR: 2016, TYPE: long, TITLE: One for All: Towards Language Independent Named Entity Linking, SUMMARY: Entity linking (EL) is the task of disambiguating mentions in text by associating them with entries in a predefined database of mentions (persons, organizations, etc). Most previous EL research has focused mainly on one language, English, with less attention being paid to other languages, such as Spanish or Chinese. In this paper, we introduce LIEL, a Language Independent Entity Linking system, which provides an EL framework which, once trained on one language, works remarkably well on a number of different languages without change. LIEL makes a joint global prediction over the entire document, employing a discriminative reranking framework with many domain and language-independent feature functions. Experiments on numerous benchmark datasets, show that the proposed system, once trained on one language, English, outperforms several state-of-the-art systems in English (by 4 points) and the trained model also works very well on Spanish (14 points better than a competitor system), demonstrating the viability of the approach.

--- AUTHORS ---
PUBLICATIONID: 1, AUTHOR: Hannah Rashkin
PUBLICATIONID: 1, AUTHOR: Sameer Singh
PUBLICATIONID: 1, AUTHOR: Yejin Choi
PUBLICATIONID: 10, AUTHOR: Abhinav Rastogi
PUBLICATIONID: 10, AUTHOR: Christopher D. Manning
PUBLICATIONID: 10, AUTHOR: Christopher Potts
PUBLICATIONID: 10, AUTHOR: Jon Gauthier
PUBLICATIONID: 10, AUTHOR: Raghav Gupta
PUBLICATIONID: 10, AUTHOR: Samuel R. Bowman
PUBLICATIONID: 11, AUTHOR: Alessandro Presta
PUBLICATIONID: 11, AUTHOR: Aliaksei Severyn
PUBLICATIONID: 11, AUTHOR: Chris Alberti
PUBLICATIONID: 11, AUTHOR: Daniel Andor
PUBLICATIONID: 11, AUTHOR: David Weiss
PUBLICATIONID: 11, AUTHOR: Kuzman Ganchev
PUBLICATIONID: 11, AUTHOR: Michael Collins
PUBLICATIONID: 11, AUTHOR: Slav Petrov
PUBLICATIONID: 12, AUTHOR: Eduard Hovy
PUBLICATIONID: 12, AUTHOR: Eric Xing
PUBLICATIONID: 12, AUTHOR: Xuezhe Ma
PUBLICATIONID: 12, AUTHOR: Zhengzhong Liu
PUBLICATIONID: 12, AUTHOR: Zhiting Hu
PUBLICATIONID: 13, AUTHOR: Hang Li
PUBLICATIONID: 13, AUTHOR: Jiatao Gu
PUBLICATIONID: 13, AUTHOR: Victor O.K. Li
PUBLICATIONID: 13, AUTHOR: Zhengdong Lu
PUBLICATIONID: 14, AUTHOR: Andrew Senior
PUBLICATIONID: 14, AUTHOR: Edward Grefenstette
PUBLICATIONID: 14, AUTHOR: Fumin Wang
PUBLICATIONID: 14, AUTHOR: Karl Moritz Hermann
PUBLICATIONID: 14, AUTHOR: Phil Blunsom
PUBLICATIONID: 14, AUTHOR: Tom Ko?isk
PUBLICATIONID: 14, AUTHOR: Wang Ling
PUBLICATIONID: 15, AUTHOR: Jianpeng Cheng
PUBLICATIONID: 15, AUTHOR: Mirella Lapata
PUBLICATIONID: 16, AUTHOR: Ardavan Saeedi
PUBLICATIONID: 16, AUTHOR: Karthik Narasimhan
PUBLICATIONID: 16, AUTHOR: Kayhan Batmanghelich
PUBLICATIONID: 16, AUTHOR: Sam Gershman
PUBLICATIONID: 17, AUTHOR: Karl Pichotta
PUBLICATIONID: 17, AUTHOR: Raymond J. Mooney
PUBLICATIONID: 18, AUTHOR: Boris Katz
PUBLICATIONID: 18, AUTHOR: Carolyn Spadine
PUBLICATIONID: 18, AUTHOR: Jessica Kenney
PUBLICATIONID: 18, AUTHOR: Jing Xian Wang
PUBLICATIONID: 18, AUTHOR: Keiko Sophie Mori
PUBLICATIONID: 18, AUTHOR: Lucia Lam
PUBLICATIONID: 18, AUTHOR: Sebastian Garza
PUBLICATIONID: 18, AUTHOR: Yevgeni Berzak
PUBLICATIONID: 19, AUTHOR: Aurelien Waite
PUBLICATIONID: 19, AUTHOR: Bill Byrne
PUBLICATIONID: 19, AUTHOR: Eva Hasler
PUBLICATIONID: 19, AUTHOR: Felix Stahlberg
PUBLICATIONID: 2, AUTHOR: Alexandra Birch
PUBLICATIONID: 2, AUTHOR: Barry Haddow
PUBLICATIONID: 2, AUTHOR: Rico Sennrich
PUBLICATIONID: 20, AUTHOR: Michael Roth
PUBLICATIONID: 20, AUTHOR: Mirella Lapata
PUBLICATIONID: 21, AUTHOR: Bhuwan Dhingra
PUBLICATIONID: 21, AUTHOR: Hanxiao Liu
PUBLICATIONID: 21, AUTHOR: Ruslan Salakhutdinov
PUBLICATIONID: 21, AUTHOR: William Cohen
PUBLICATIONID: 21, AUTHOR: Zhilin Yang
PUBLICATIONID: 22, AUTHOR: Christopher D. Manning
PUBLICATIONID: 22, AUTHOR: Percy Liang
PUBLICATIONID: 22, AUTHOR: Sida I. Wang
PUBLICATIONID: 23, AUTHOR: Jessica Ficler
PUBLICATIONID: 23, AUTHOR: Yoav Goldberg
PUBLICATIONID: 24, AUTHOR: Chaitanya Shivade
PUBLICATIONID: 24, AUTHOR: Preethi Raghavan
PUBLICATIONID: 24, AUTHOR: Siddharth Patwardhan
PUBLICATIONID: 25, AUTHOR: Percy Liang
PUBLICATIONID: 25, AUTHOR: Robin Jia
PUBLICATIONID: 26, AUTHOR: Blaise Thomson
PUBLICATIONID: 26, AUTHOR: Diarmuid  Saghdha
PUBLICATIONID: 26, AUTHOR: Nikola Mrki
PUBLICATIONID: 26, AUTHOR: Steve Young
PUBLICATIONID: 26, AUTHOR: Tsung-Hsien Wen
PUBLICATIONID: 27, AUTHOR: Dimitrios Alikaniotis
PUBLICATIONID: 27, AUTHOR: Helen Yannakoudakis
PUBLICATIONID: 27, AUTHOR: Marek Rei
PUBLICATIONID: 28, AUTHOR: Alexey Borisov
PUBLICATIONID: 28, AUTHOR: Maarten de Rijke
PUBLICATIONID: 28, AUTHOR: Tom Kenter
PUBLICATIONID: 29, AUTHOR: Dan Roth
PUBLICATIONID: 29, AUTHOR: Haoruo Peng
PUBLICATIONID: 3, AUTHOR: Ji He
PUBLICATIONID: 3, AUTHOR: Jianfeng Gao
PUBLICATIONID: 3, AUTHOR: Jianshu Chen
PUBLICATIONID: 3, AUTHOR: Li Deng
PUBLICATIONID: 3, AUTHOR: Lihong Li
PUBLICATIONID: 3, AUTHOR: Mari Ostendorf
PUBLICATIONID: 3, AUTHOR: Xiaodong He
PUBLICATIONID: 30, AUTHOR: Angeliki Lazaridou
PUBLICATIONID: 30, AUTHOR: Denis Paperno
PUBLICATIONID: 30, AUTHOR: Gemma Boleda
PUBLICATIONID: 30, AUTHOR: Germn Kruszewski
PUBLICATIONID: 30, AUTHOR: Marco Baroni
PUBLICATIONID: 30, AUTHOR: Ngoc Quan Pham
PUBLICATIONID: 30, AUTHOR: Raffaella Bernardi
PUBLICATIONID: 30, AUTHOR: Raquel Fernandez
PUBLICATIONID: 31, AUTHOR: James Cross
PUBLICATIONID: 31, AUTHOR: Liang Huang
PUBLICATIONID: 32, AUTHOR: Panupong Pasupat
PUBLICATIONID: 32, AUTHOR: Percy Liang
PUBLICATIONID: 33, AUTHOR: Hinrich Schtze
PUBLICATIONID: 33, AUTHOR: Yadollah Yaghoobzadeh
PUBLICATIONID: 34, AUTHOR: Eric Xing
PUBLICATIONID: 34, AUTHOR: Hao Zhang
PUBLICATIONID: 34, AUTHOR: Mrinmaya Sachan
PUBLICATIONID: 34, AUTHOR: Yuntian Deng
PUBLICATIONID: 34, AUTHOR: Zhicheng Yan
PUBLICATIONID: 34, AUTHOR: Zhiting Hu
PUBLICATIONID: 35, AUTHOR: Ella Rabinovich
PUBLICATIONID: 35, AUTHOR: Noam Ordan
PUBLICATIONID: 35, AUTHOR: Sergiu Nisioi
PUBLICATIONID: 35, AUTHOR: Shuly Wintner
PUBLICATIONID: 36, AUTHOR: Diana Inkpen
PUBLICATIONID: 36, AUTHOR: Hui Jiang
PUBLICATIONID: 36, AUTHOR: Qian Chen
PUBLICATIONID: 36, AUTHOR: Si Wei
PUBLICATIONID: 36, AUTHOR: Xiaodan Zhu
PUBLICATIONID: 36, AUTHOR: Zhen-Hua Ling
PUBLICATIONID: 37, AUTHOR: Ge Li
PUBLICATIONID: 37, AUTHOR: Lili Mou
PUBLICATIONID: 37, AUTHOR: Yan Xu
PUBLICATIONID: 37, AUTHOR: Yunchuan Chen
PUBLICATIONID: 37, AUTHOR: Zhi Jin
PUBLICATIONID: 38, AUTHOR: Roee Aharoni
PUBLICATIONID: 38, AUTHOR: Yoav Goldberg
PUBLICATIONID: 39, AUTHOR: Abhijit Mishra
PUBLICATIONID: 39, AUTHOR: Diptesh Kanojia
PUBLICATIONID: 39, AUTHOR: Kuntal Dey
PUBLICATIONID: 39, AUTHOR: Pushpak Bhattacharyya
PUBLICATIONID: 39, AUTHOR: Seema Nagar
PUBLICATIONID: 4, AUTHOR: Alexandra Birch
PUBLICATIONID: 4, AUTHOR: Barry Haddow
PUBLICATIONID: 4, AUTHOR: Rico Sennrich
PUBLICATIONID: 40, AUTHOR: Daniel Gildea
PUBLICATIONID: 40, AUTHOR: Linfeng Song
PUBLICATIONID: 40, AUTHOR: Xiaochang Peng
PUBLICATIONID: 40, AUTHOR: Yue Zhang
PUBLICATIONID: 40, AUTHOR: Zhiguo Wang
PUBLICATIONID: 41, AUTHOR: Noah A. Smith
PUBLICATIONID: 41, AUTHOR: Yangfeng Ji
PUBLICATIONID: 42, AUTHOR: Pramod Viswanath
PUBLICATIONID: 42, AUTHOR: Suma Bhat
PUBLICATIONID: 42, AUTHOR: Tarek Sakakini
PUBLICATIONID: 43, AUTHOR: Akiko Eriguchi
PUBLICATIONID: 43, AUTHOR: Kyunghyun Cho
PUBLICATIONID: 43, AUTHOR: Yoshimasa Tsuruoka
PUBLICATIONID: 44, AUTHOR: Adam Fisch
PUBLICATIONID: 44, AUTHOR: Antoine Bordes
PUBLICATIONID: 44, AUTHOR: Danqi Chen
PUBLICATIONID: 44, AUTHOR: Jason Weston
PUBLICATIONID: 45, AUTHOR: Hinrich Schtze
PUBLICATIONID: 45, AUTHOR: Katharina Kann
PUBLICATIONID: 45, AUTHOR: Ryan Cotterell
PUBLICATIONID: 46, AUTHOR: Anders Sgaard
PUBLICATIONID: 46, AUTHOR: Isabelle Augenstein
PUBLICATIONID: 47, AUTHOR: Ari Rappoport
PUBLICATIONID: 47, AUTHOR: Daniel Hershcovich
PUBLICATIONID: 47, AUTHOR: Omri Abend
PUBLICATIONID: 48, AUTHOR: Graham Neubig
PUBLICATIONID: 48, AUTHOR: Pengcheng Yin
PUBLICATIONID: 49, AUTHOR: Fahim Dalvi
PUBLICATIONID: 49, AUTHOR: Hassan Sajjad
PUBLICATIONID: 49, AUTHOR: James Glass
PUBLICATIONID: 49, AUTHOR: Nadir Durrani
PUBLICATIONID: 49, AUTHOR: Yonatan Belinkov
PUBLICATIONID: 5, AUTHOR: Hua Wu
PUBLICATIONID: 5, AUTHOR: Maosong Sun
PUBLICATIONID: 5, AUTHOR: Shiqi Shen
PUBLICATIONID: 5, AUTHOR: Wei He
PUBLICATIONID: 5, AUTHOR: Yang Liu
PUBLICATIONID: 5, AUTHOR: Yong Cheng
PUBLICATIONID: 5, AUTHOR: Zhongjun He
PUBLICATIONID: 50, AUTHOR: Chieh Lo
PUBLICATIONID: 50, AUTHOR: Frederick Liu
PUBLICATIONID: 50, AUTHOR: Graham Neubig
PUBLICATIONID: 50, AUTHOR: Han Lu
PUBLICATIONID: 51, AUTHOR: Eduard Hovy
PUBLICATIONID: 51, AUTHOR: Qizhe Xie
PUBLICATIONID: 51, AUTHOR: Xuezhe Ma
PUBLICATIONID: 51, AUTHOR: Zihang Dai
PUBLICATIONID: 52, AUTHOR: Iryna Gurevych
PUBLICATIONID: 52, AUTHOR: Johannes Daxenberger
PUBLICATIONID: 52, AUTHOR: Steffen Eger
PUBLICATIONID: 53, AUTHOR: Bing Xiang
PUBLICATIONID: 53, AUTHOR: Bowen Zhou
PUBLICATIONID: 53, AUTHOR: Cicero dos Santos
PUBLICATIONID: 53, AUTHOR: Kazi Saidul Hasan
PUBLICATIONID: 53, AUTHOR: Mo Yu
PUBLICATIONID: 53, AUTHOR: Wenpeng Yin
PUBLICATIONID: 54, AUTHOR: Artem Sokolov
PUBLICATIONID: 54, AUTHOR: Julia Kreutzer
PUBLICATIONID: 54, AUTHOR: Stefan Riezler
PUBLICATIONID: 55, AUTHOR: David Weir
PUBLICATIONID: 55, AUTHOR: Jeremy Reffin
PUBLICATIONID: 55, AUTHOR: Julie Weeds
PUBLICATIONID: 55, AUTHOR: Thomas Kober
PUBLICATIONID: 56, AUTHOR: Michael Strube
PUBLICATIONID: 56, AUTHOR: Nafise Sadat Moosavi
PUBLICATIONID: 57, AUTHOR: Claire Cardie
PUBLICATIONID: 57, AUTHOR: Joonsuk Park
PUBLICATIONID: 57, AUTHOR: Vlad Niculae
PUBLICATIONID: 58, AUTHOR: Adams Wei Yu
PUBLICATIONID: 58, AUTHOR: Hongrae Lee
PUBLICATIONID: 58, AUTHOR: Quoc Le
PUBLICATIONID: 59, AUTHOR: Daqing He
PUBLICATIONID: 59, AUTHOR: Rui Meng
PUBLICATIONID: 59, AUTHOR: Sanqiang Zhao
PUBLICATIONID: 59, AUTHOR: Shuguang Han
PUBLICATIONID: 6, AUTHOR: David Grangier
PUBLICATIONID: 6, AUTHOR: Michael Auli
PUBLICATIONID: 6, AUTHOR: Wenlin Chen
PUBLICATIONID: 60, AUTHOR: Graham Neubig
PUBLICATIONID: 60, AUTHOR: Koichiro Yoshino
PUBLICATIONID: 60, AUTHOR: Philip Arthur
PUBLICATIONID: 60, AUTHOR: Satoshi Nakamura
PUBLICATIONID: 60, AUTHOR: Yusuke Oda
PUBLICATIONID: 61, AUTHOR: Christopher D. Manning
PUBLICATIONID: 61, AUTHOR: Percy Liang
PUBLICATIONID: 61, AUTHOR: Sam Ginn
PUBLICATIONID: 61, AUTHOR: Sida I. Wang
PUBLICATIONID: 62, AUTHOR: Anca Dragan
PUBLICATIONID: 62, AUTHOR: Dan Klein
PUBLICATIONID: 62, AUTHOR: Jacob Andreas
PUBLICATIONID: 63, AUTHOR: Chris Dyer
PUBLICATIONID: 63, AUTHOR: Kartik Goyal
PUBLICATIONID: 63, AUTHOR: Taylor Berg-Kirkpatrick
PUBLICATIONID: 64, AUTHOR: Benjamin Strauss
PUBLICATIONID: 64, AUTHOR: Michael Bloodgood
PUBLICATIONID: 65, AUTHOR: Furu Wei
PUBLICATIONID: 65, AUTHOR: Ming Zhou
PUBLICATIONID: 65, AUTHOR: Nan Yang
PUBLICATIONID: 65, AUTHOR: Qingyu Zhou
PUBLICATIONID: 66, AUTHOR: Jan Buys
PUBLICATIONID: 66, AUTHOR: Phil Blunsom
PUBLICATIONID: 67, AUTHOR: Boris Katz
PUBLICATIONID: 67, AUTHOR: Chie Nakamura
PUBLICATIONID: 67, AUTHOR: Suzanne Flynn
PUBLICATIONID: 67, AUTHOR: Yevgeni Berzak
PUBLICATIONID: 68, AUTHOR: Dan Klein
PUBLICATIONID: 68, AUTHOR: Maxim Rabinovich
PUBLICATIONID: 68, AUTHOR: Mitchell Stern
PUBLICATIONID: 69, AUTHOR: Xinchi Chen
PUBLICATIONID: 69, AUTHOR: Xipeng Qiu
PUBLICATIONID: 69, AUTHOR: Xuanjing Huang
PUBLICATIONID: 69, AUTHOR: Zhan Shi
PUBLICATIONID: 7, AUTHOR: Julian Hitschler
PUBLICATIONID: 7, AUTHOR: Shigehiko Schamoni
PUBLICATIONID: 7, AUTHOR: Stefan Riezler
PUBLICATIONID: 70, AUTHOR: Dan Klein
PUBLICATIONID: 70, AUTHOR: Maxim Rabinovich
PUBLICATIONID: 71, AUTHOR: Dan Garrette
PUBLICATIONID: 71, AUTHOR: Hannah Alpert-Abrams
PUBLICATIONID: 71, AUTHOR: Maria Ryskina
PUBLICATIONID: 71, AUTHOR: Taylor Berg-Kirkpatrick
PUBLICATIONID: 72, AUTHOR: Jey Han Lau
PUBLICATIONID: 72, AUTHOR: Timothy Baldwin
PUBLICATIONID: 72, AUTHOR: Trevor Cohn
PUBLICATIONID: 73, AUTHOR: Alexander Fonarev
PUBLICATIONID: 73, AUTHOR: Gleb Gusev
PUBLICATIONID: 73, AUTHOR: Ivan Oseledets
PUBLICATIONID: 73, AUTHOR: Oleksii Hrinchuk
PUBLICATIONID: 73, AUTHOR: Pavel Serdyukov
PUBLICATIONID: 74, AUTHOR: Andrew Wilson
PUBLICATIONID: 74, AUTHOR: Ben Athiwaratkun
PUBLICATIONID: 75, AUTHOR: Fei Dong
PUBLICATIONID: 75, AUTHOR: Jie Yang
PUBLICATIONID: 75, AUTHOR: Yue Zhang
PUBLICATIONID: 76, AUTHOR: Akiko Aizawa
PUBLICATIONID: 76, AUTHOR: Hui Su
PUBLICATIONID: 76, AUTHOR: Shuzi Niu
PUBLICATIONID: 76, AUTHOR: Wenjie Li
PUBLICATIONID: 76, AUTHOR: Xiaoyu Shen
PUBLICATIONID: 76, AUTHOR: Yanran Li
PUBLICATIONID: 77, AUTHOR: John Wieting
PUBLICATIONID: 77, AUTHOR: Kevin Gimpel
PUBLICATIONID: 78, AUTHOR: Arianna Bisazza
PUBLICATIONID: 78, AUTHOR: Christof Monz
PUBLICATIONID: 78, AUTHOR: Marzieh Fadaee
PUBLICATIONID: 79, AUTHOR: Jie Zhou
PUBLICATIONID: 79, AUTHOR: Mingxuan Wang
PUBLICATIONID: 79, AUTHOR: Qun Liu
PUBLICATIONID: 79, AUTHOR: Zhengdong Lu
PUBLICATIONID: 8, AUTHOR: Eric Xing
PUBLICATIONID: 8, AUTHOR: Kumar Dubey
PUBLICATIONID: 8, AUTHOR: Mrinmaya Sachan
PUBLICATIONID: 80, AUTHOR: Jason Eisner
PUBLICATIONID: 80, AUTHOR: Ryan Cotterell
PUBLICATIONID: 81, AUTHOR: Ruochen Xu
PUBLICATIONID: 81, AUTHOR: Yiming Yang
PUBLICATIONID: 82, AUTHOR: Andi Zhang
PUBLICATIONID: 82, AUTHOR: Andrew Abel
PUBLICATIONID: 82, AUTHOR: Dong Wang
PUBLICATIONID: 82, AUTHOR: Jiyuan Zhang
PUBLICATIONID: 82, AUTHOR: Shiyue Zhang
PUBLICATIONID: 82, AUTHOR: Yang Feng
PUBLICATIONID: 82, AUTHOR: Yang Wang
PUBLICATIONID: 83, AUTHOR: Joseph Kim
PUBLICATIONID: 83, AUTHOR: Julie Shah
PUBLICATIONID: 83, AUTHOR: Kechen Qin
PUBLICATIONID: 83, AUTHOR: Lu Wang
PUBLICATIONID: 84, AUTHOR: GuangYong Leonard Chan
PUBLICATIONID: 84, AUTHOR: Hai Leong Chieu
PUBLICATIONID: 84, AUTHOR: Hongmin Wang
PUBLICATIONID: 84, AUTHOR: Jie Yang
PUBLICATIONID: 84, AUTHOR: Yue Zhang
PUBLICATIONID: 85, AUTHOR: Maxwell Forbes
PUBLICATIONID: 85, AUTHOR: Yejin Choi
PUBLICATIONID: 86, AUTHOR: Hongyu GUO
PUBLICATIONID: 87, AUTHOR: Jianfeng Gao
PUBLICATIONID: 87, AUTHOR: Jianshu Ji
PUBLICATIONID: 87, AUTHOR: Kristina Toutanova
PUBLICATIONID: 87, AUTHOR: Qinlong Wang
PUBLICATIONID: 87, AUTHOR: Steven Truong
PUBLICATIONID: 87, AUTHOR: Yongen Gong
PUBLICATIONID: 88, AUTHOR: Kentaro Inui
PUBLICATIONID: 88, AUTHOR: Naoaki Okazaki
PUBLICATIONID: 88, AUTHOR: Sho Takase
PUBLICATIONID: 89, AUTHOR: Adam Lopez
PUBLICATIONID: 89, AUTHOR: Jianpeng Cheng
PUBLICATIONID: 89, AUTHOR: Mirella Lapata
PUBLICATIONID: 9, AUTHOR: Jan Kleindienst
PUBLICATIONID: 9, AUTHOR: Martin Schmid
PUBLICATIONID: 9, AUTHOR: Ondej Bajgar
PUBLICATIONID: 9, AUTHOR: Rudolf Kadlec
PUBLICATIONID: 90, AUTHOR: Bing Xiang
PUBLICATIONID: 90, AUTHOR: Bowen Zhou
PUBLICATIONID: 90, AUTHOR: Liang Huang
PUBLICATIONID: 90, AUTHOR: Mingbo Ma
PUBLICATIONID: 91, AUTHOR: Avirup Sil
PUBLICATIONID: 91, AUTHOR: Radu Florian

Menu:
1. View table contents
2. Search by PUBLICATIONID
3. Search by one or more attributes
4. Exit
Enter your choice: 2
Enter PUBLICATIONID: 27

--- SEARCH RESULT ---
PUBLICATIONID: 27, YEAR: 2016, TYPE: long, TITLE: Automatic Text Scoring Using Neural Networks, SUMMARY: Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.

Menu:
1. View table contents
2. Search by PUBLICATIONID
3. Search by one or more attributes
4. Exit
Enter your choice: 3
Input fields:
AUTHOR: Marek Rei
TITLE: Automatic Text Scoring Using Neural Networks
YEAR: 2016
TYPE: long
Output fields:
PUBLICATIONID (Yes/No): yes
AUTHOR (Yes/No): yes
TITLE (Yes/No): yes
YEAR (Yes/No): yes
TYPE (Yes/No): yes
SUMMARY (Yes/No): yes
Sorted by: author
Query: SELECT P.PUBLICATIONID, AUTHOR, TITLE, YEAR, TYPE, SUMMARY  FROM PUBLICATIONS P JOIN AUTHORS A ON P.PUBLICATIONID = A.PUBLICATIONID WHERE UPPER(A.AUTHOR) LIKE UPPER(?) AND UPPER(P.TITLE) LIKE UPPER(?) AND P.YEAR = ? AND UPPER(P.TYPE) LIKE UPPER(?) ORDER BY author

--- SEARCH RESULTS ---
PUBLICATIONID: 27, AUTHOR: Marek Rei, TITLE: Automatic Text Scoring Using Neural Networks, YEAR: 2016, TYPE: long, SUMMARY: Automated Text Scoring (ATS) provides a cost-effective and consistent alternative to human marking. However, in order to achieve good performance, the predictive features of the system need to be manually engineered by human experts. We introduce a model that forms word representations by learning the extent to which specific words contribute to the text's score. Using Long-Short Term Memory networks to represent the meaning of texts, we demonstrate that a fully automated framework is able to achieve excellent results over similar approaches. In an attempt to make our results more interpretable, and inspired by recent advances in visualizing neural networks, we introduce a novel method for identifying the regions of the text that the model has found more discriminative.

Menu:
1. View table contents
2. Search by PUBLICATIONID
3. Search by one or more attributes
4. Exit
Enter your choice: 4
Exiting program...
